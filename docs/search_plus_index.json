{"./":{"url":"./","title":"Introduction","keywords":"","body":"druler "},"知识笔记/":{"url":"知识笔记/","title":"知识笔记","keywords":"","body":"druler "},"知识笔记/IDE/idea/idea.html":{"url":"知识笔记/IDE/idea/idea.html","title":"Idea","keywords":"","body":"idea project structure https://www.cnblogs.com/jajian/p/8081640.html "},"知识笔记/IDE/vscode/vscode.html":{"url":"知识笔记/IDE/vscode/vscode.html","title":"Vscode","keywords":"","body":""},"知识笔记/java/maven/maven编译换行符问题.html":{"url":"知识笔记/java/maven/maven编译换行符问题.html","title":"maven编译换行符问题","keywords":"","body":"解决方案： mavem添加插件antrun，配置fixcrlf org.apache.maven.plugins maven-antrun-plugin ant-test package run 参考： https://stackoverflow.com/questions/2162275/convert-files-to-unix-format-using-maven http://www.iteye.com/problems/69937 跳过测试 -DskipTests # 不执行测试用例，但编译测试用例类生成相应的 class 文件至 target/test-classes 下 -Dmaven.test.skip=true # 不执行测试用例，也不编译测试用例类 例如 mvn clean install -DskipTests mvn clean install -Dmaven.test.skip=true 从指定模块编译 -rf :moduleName # 从 moduleName 模块开始编译 在编译到一半后，报错退出。使用这个参数指定从上次编译失败的模块处开始编译。不用从头开始编译，可以避免浪费不必要的时间 例如编译flink 时，从 flink-hadoop-fs 模块开始编译 mvn -rf :flink-hadoop-fs clean install 并行构建 使用多个线程，并行构建相互之间没有依赖关系的模块 4 表示用4个线程构建 $ mvn -T 4 clean install 1C 表示为机器的每个 core分配一个线程，如何4核4线程的机器，就是 1*4 个线程 $ mvn -T 1C clean install 跳过失败的模块，编译到最后再报错 mvn clean install --fail-at-end ———————————————— 版权声明：本文为CSDN博主「好笨的菜鸟」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/qq_38976805/article/details/103066020 ranger-hive-plugin "},"知识笔记/java/spring/spring.html":{"url":"知识笔记/java/spring/spring.html","title":"Spring","keywords":"","body":"spring 参考文档 https://www.jianshu.com/p/210e8e1b5b53 IOC 控制反转， 人话：提需求，找代理，代理帮你搞需求 AOP 面向截面 人话：设计好边界，面向边界写 spring mvc 自带mvc? springboot 注解会给使用者提供方便 对第三方技术进行了很好的封装和整合，提供了大量第三方接口 可以通过依赖自动配置，不需要XML等配置文件 提供了安全等特性 spring cloud springcloud是微服务解决方案，非常方便，是基于springboot的。 开始搞idea 了 搞了字符集, 用 u8 spring boot hello world 参考文档 https://www.jianshu.com/p/68d8a202c38b module 里选 spring initializr maven 一顿抽拉会建一个spring boot 的项目 核心的文件： pom.xml ​ application.properties/application.yml ​ Application.java (项目的入口文件) ​ controller 文件夹（与application 同级） 感觉vue 要学起来 "},"知识笔记/linux/ftp/ftp.html":{"url":"知识笔记/linux/ftp/ftp.html","title":"Ftp","keywords":"","body":"ftp 安装服务 #安装 yum install -y vsftpd #设置开机启动 systemctl enable vsftpd.service #启动 systemctl start vsftpd.service #停止 systemctl stop vsftpd.service #查看状态 systemctl status vsftpd.service ftp创建用户 useradd -d /ftp_data -g ftp -s /sbin/nologin ftp_test1 passwd ftp_test1 --设置密码 ## echo \"qwert\" | passwd --stdin rusky 创建时对于共享数据 应归到同一个组里 "},"知识笔记/linux/kereros/cdh安装kerberos.html":{"url":"知识笔记/linux/kereros/cdh安装kerberos.html","title":"cdh安装kerberos","keywords":"","body":"Kerberos 部署 kerberos 参考文档 https://blog.csdn.net/ytp552200ytp/article/details/109643832 应用程序 使用 kerberos https://blog.csdn.net/qq_42790479/article/details/104040870 看了一个装 cdh 的 https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&mid=2247495280&idx=1&sn=e1d09b47cc18a2d6e862d21501b44475&chksm=ec293e79db5eb76f6267a8ac4309b920e291f2f5c792eb02a64feec6af72892d625fdf4c1085&scene=21#wechat_redirect 对应装 kerberos 的 https://blog.csdn.net/yangbosos/article/details/88718135 KDC服务安装及配置 本文档中将KDC服务安装在Cloudera Manager Server所在服务器上（KDC服务可根据自己需要安装在其他服务器） 1.在Cloudera Manager服务器上安装KDC服务 [root@ip-172-31-6-83 ~]# yum -y install krb5-server krb5-libs krb5-auth-dialog krb5-workstation 2.修改/etc/krb5.conf配置 [root@ip-172-31-6-83 ~]# vim /etc/krb5.conf # Configuration snippets may be placed in this directory as well includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = FAYSON.COM #default_ccache_name = KEYRING:persistent:%{uid} [realms] FAYSON.COM = { kdc = ip-172-31-6-83.ap-southeast-1.compute.internal admin_server = ip-172-31-6-83.ap-southeast-1.compute.internal } [domain_realm] .ap-southeast-1.compute.internal = FAYSON.COM ap-southeast-1.compute.internal = FAYSON.COM 标红部分为需要修改的信息。 3.修改/var/kerberos/krb5kdc/kadm5.acl配置 [root@ip-172-31-6-83 ~]# vim /var/kerberos/krb5kdc/kadm5.acl */admin@FAYSON.COM * 4.修改/var/kerberos/krb5kdc/kdc.conf配置 [root@ip-172-31-6-83 ~]# vim /var/kerberos/krb5kdc/kdc.conf [root@ip-172-31-6-83 ~]# cat /var/kerberos/krb5kdc/kdc.conf [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] FAYSON.COM = { #master_key_type = aes256-cts max_renewable_life= 7d 0h 0m 0s acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } 标红部分为需要修改的配置。 5.创建Kerberos数据库 [root@ip-172-31-6-83 ~]# kdb5_util create –r FAYSON.COM -s Loading random data Initializing database '/var/kerberos/krb5kdc/principal' for realm 'FAYSON.COM', master key name 'K/M@FAYSON.COM' You will be prompted for the database Master Password. It is important that you NOT FORGET this password. Enter KDC database master key: Re-enter KDC database master key to verify: 此处需要输入Kerberos数据库的密码。 6.创建Kerberos的管理账号 [root@ip-172-31-6-83 ~]# kadmin.local Authenticating as principal root/admin@FAYSON.COM with password. kadmin.local: addprinc admin/admin@FAYSON.COM WARNING: no policy specified for admin/admin@FAYSON.COM; defaulting to no policy Enter password for principal \"admin/admin@FAYSON.COM\": Re-enter password for principal \"admin/admin@FAYSON.COM\": Principal \"admin/admin@FAYSON.COM\" created. kadmin.local: exit 标红部分为Kerberos管理员账号，需要输入管理员密码。 7.将Kerberos服务添加到自启动服务，并启动krb5kdc和kadmin服务 [root@ip-172-31-6-83 ~]# systemctl enable krb5kdc Created symlink from /etc/systemd/system/multi-user.target.wants/krb5kdc.service to /usr/lib/systemd/system/krb5kdc.service. [root@ip-172-31-6-83 ~]# systemctl enable kadmin Created symlink from /etc/systemd/system/multi-user.target.wants/kadmin.service to /usr/lib/systemd/system/kadmin.service. [root@ip-172-31-6-83 ~]# systemctl start krb5kdc [root@ip-172-31-6-83 ~]# systemctl start kadmin 8.测试Kerberos的管理员账号 [root@ip-172-31-6-83 ~]# kinit admin/admin@FAYSON.COM Password for admin/admin@FAYSON.COM: [root@ip-172-31-6-83 ~]# klist Ticket cache: FILE:/tmp/krb5cc_0 Default principal: admin/admin@FAYSON.COM Valid starting Expires Service principal 12/27/2018 22:05:56 12/28/2018 22:05:56 krbtgt/FAYSON.COM@FAYSON.COM renew until 01/03/2019 22:05:56 9.为集群安装所有Kerberos客户端，包括Cloudera Manager 使用批处理脚本为集群所有节点安装Kerberos客户端 [root@ip-172-31-6-83 shell]# sh ssh_do_all.sh node.list 'yum -y install krb5-libs krb5-workstation' 10.在Cloudera Manager Server服务器上安装额外的包 [root@ip-172-31-6-83 shell]# yum -y install openldap-clients 11.将KDC Server上的krb5.conf文件拷贝到所有Kerberos客户端 使用批处理脚本将Kerberos服务端的krb5.conf配置文件拷贝至集群所有节点的/etc目录下： [root@ip-172-31-6-83 shell]# sh bk_cp.sh node.list /etc/krb5.conf /etc/ 3 "},"知识笔记/linux/LDAP/LDAP.html":{"url":"知识笔记/linux/LDAP/LDAP.html","title":"LDAP","keywords":"","body":"LADP https://www.cnblogs.com/wilburxu/p/9174353.html "},"知识笔记/linux/开启图形化页面.html":{"url":"知识笔记/linux/开启图形化页面.html","title":"开启图形化页面","keywords":"","body":"Centos开启图形化页面 磁盘挂载 ## 主要命令 fdisk /dev/sdb 等 https://blog.csdn.net/qq_30604989/article/details/81163270 mkdir /var/iso mount -o loop CentOS-6.7-x86_64-bin-DVD1.iso /var/iso 配置本地源 ## yum源需要httpd服务 yum instll -y httpd 挂载iso 安装 X 窗口系统 yum groupinstall \"X Window System\" 安装图形界面软件 GNOME yum groupinstall \"GNOME Desktop\" \"Graphical Administration Tools\" yum groupinstall \"GNOME Desktop\" --skip-broken yum groupinstall \"Graphical Administration Tools\" 开启vnc远程 https://blog.csdn.net/qq_14898661/article/details/87252960 "},"知识笔记/linux/离线环境使用yum.html":{"url":"知识笔记/linux/离线环境使用yum.html","title":"离线环境使用yum","keywords":"","body":"离线部署yum依赖 利用本地源解决在无网环境部署应用需要解决的问题： 应用需要哪些软件包？ 如何把应用依赖的软件包制作成一个精简的本地源? 如何使用本地源? 第一个问题使用yum-utils解决，它带的repotrack 命令可以把应用所依赖的软件包全部下载到本地，安装： yum install -y yum-utils # repotrack 工具用来下载yum依赖 以要离线部署gcc为例来下载所有相关依赖： mkdir -p /home/oaksharks/install/yumRepo/packages repotrack gcc -p /home/oaksharks/install/yumRepo/packages repotrack 下载的都是 rpm包，如果用rpm安装不容易解决依赖关系，可以给这些包生成索引作为一个本地的yum源，可以使用createrepo完成 先安装： yum install -y createrepo # 使用createrepo 创建私有yum源 给rpm包创建索引： createrepo /home/oaksharks/install/yumRepo/packages 索引会创建到createrepo /home/oaksharks/install/yumRepo/packages/repo目录。 配置使用本地源： 创建文件/etc/yum.repos.d/CentOS-Local.repo 内容为： [Local] name=Local Yum baseurl=file:///home/oaksharks/install/yumRepo/packages gpgcheck=0 enabled=1 软件重建缓存： yum clean all yum makecache 可以使用yum利用本地源安装软件了： yum install -y gcc 常见问题 怎么新加软件包？ 使用repotrack 下载新的包 删除 repo目录，然后重新创建索引 已经验证此种方式切断网卡后可以正常使用，可放心食用。 "},"知识笔记/linux/软链接.html":{"url":"知识笔记/linux/软链接.html","title":"软链接","keywords":"","body":" ssh 22.86.180.239 \" ## 删除 rm -rf /data/bdpsolid ; rm -rf /bdp ; ## 创建 mkdir -p /data/bdpsolid ;mkdir -p /bdp ; ## 把东西连上 ln -s /data/bdpsolid /bdp/data ; ## 给授权 chown -R bdp:bdp /data/ \" "},"知识笔记/web/html/html.html":{"url":"知识笔记/web/html/html.html","title":"Html","keywords":"","body":"html markdown 可以直接支持 html 所以可以 直接贴进来展示 展示效果 可以更好的为表现形式提供服务 地址栏 Written by Jon Doe. Visit us at: Example.com Box 564, Disneyland USA * 中文 Written by Jon Doe. Visit us at: Example.com q企业模型 勒沃库森呢 对于markdown 可能是 typora 的问题 对于大段的html 中间的空行会造成影响，直接贴过来效果 Look! Styles and colors Look! Styles and colors ​ Manipulate Text Colors Boxes and more... and more... and more... 把空行去掉的实际应该的效果 Look! Styles and colors Look! Styles and colors Manipulate Text Colors Boxes and more... and more... and more... etc. 菜鸟教程(runoob.com) 我使用了外部样式文件来格式化文本 我也是! 我使用了外部样式文件来格式化文本 我2222使用了外部样式文件来格式化文本 我2222使用了外部样式文件来 我也是!"},"知识笔记/web/html/基础概念.html":{"url":"知识笔记/web/html/基础概念.html","title":"基础概念","keywords":"","body":"作者：Hap Xip 链接：https://www.zhihu.com/question/34219998/answer/223720383 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 首先得温习一点，一般网页都是用HTML编写的，而HTML采用标签 声明各个结构（如下图），如这里是段落在浏览器中打开就是一段写着“这里是段落”的一段话。 同时HTML允许结构的嵌套，也就是说你可以在段落之间插入其它标签，比如分割线。写网页（HTML）就好比绘制（罪犯）肖像，分别从头部和身体两大部分逐一描述（对应HTML文档里的head、body），再详细描述某部分的细节，如头部可从头发、脸部、脖子入手，再详细点，如脸部可再分眼睛、耳朵、鼻子、嘴巴等去描述（对应 HTML 文档里的各种层级的嵌套）。我们暂且称这个被描述的人为小明（后边会再提到这个人）。 至此，静态网页的编写就完成了。静态网页没有任何交互行为，相当于一个麻木的人，没有“反射神经”，怎么欺负Ta，也不会哭，更不会还手，于是人们引入了 JS（JavaScript）。它的职责就是为了让页面“活起来”。 这时候就可以设置页面的交互行为了，比如拍打小明的手臂，皮肤会发紫（对应 HTML 文档中点击某个按钮时指定的段落会改变字体颜色）。这就存在一个对象的指定问题：计算机只听得懂准确详细的描述语言。刚刚说的“拍打小明的手臂，皮肤会发紫”我们能自动理解成“拍打小明的手臂，手臂上被拍打的皮肤会发紫”，计算机没那么聪明呃，不会自动补全，需要你一五一十地告诉它，是手臂上的、被拍打过的皮肤会发紫，而不是全身皮肤(Q_Q)或其它地方。 OK～到这里就可以引入 DOM 这个概念了，你可以给某个具体的部位命名，比如皮肤Ⅰ区、皮肤ⅠⅠ区、皮肤ⅠⅠⅠ区、皮肤Ⅳ区……（对应HTML文档里某个标签结构声明 id、class 等）。设置交互行为的时候就可以方便地指出是哪个部分做出什么反应了（对应HTML文档里 getElementById 等）。这时候小明就能做复杂的动作，比如敲左膝盖抬右脚的反常规行为⊙ω⊙。 发布于 2017-09-01 "},"知识笔记/办公软件/excel/目录导航宏.html":{"url":"知识笔记/办公软件/excel/目录导航宏.html","title":"目录导航宏","keywords":"","body":"目录导航宏 http://blog.sina.com.cn/s/blog_a63df91c0102z3p3.html alt + f11 2、插入-模版 Sub mulu() On Error GoTo Tuichu Dim i As Integer Dim ShtCount As Integer Dim SelectionCell As Range ShtCount = Worksheets.Count If ShtCount = 0 Or ShtCount = 1 Then Exit Sub Application.ScreenUpdating = False For i = 1 To ShtCount If Sheets(i).Name = \"目录\" Then Sheets(\"目录\").Move Before:=Sheets(1) End If Next i If Sheets(1).Name <> \"目录\" Then ShtCount = ShtCount + 1 Sheets(1).Select Sheets.Add Sheets(1).Name = \"目录\" End If Sheets(\"目录\").Select Columns(\"B:B\").Delete Shift:=xlToLeft Application.StatusBar = \"正在生成目录…………请等待！\" For i = 2 To ShtCount ActiveSheet.Hyperlinks.Add Anchor:=Worksheets(\"目录\").Cells(i, 2), Address:=\"\", SubAddress:= _ \"'\" & Sheets(i).Name & \"'!R1C1\", TextToDisplay:=Sheets(i).Name Next Sheets(\"目录\").Select Columns(\"B:B\").AutoFit Cells(1, 2) = \"目录\" Set SelectionCell = Worksheets(\"目录\").Range(\"B1\") With SelectionCell .HorizontalAlignment = xlDistributed .VerticalAlignment = xlCenter .AddIndent = True .Font.Bold = True .Interior.ColorIndex = 34 End With Application.StatusBar = False Application.ScreenUpdating = True Tuichu: End Sub f5 运行刷新 "},"知识笔记/大数据组件/hive/hive_on_spark.html":{"url":"知识笔记/大数据组件/hive/hive_on_spark.html","title":"Hive On Spark","keywords":"","body":"set hive.execution.engine=spark; set hive.spark.client.future.timeout=200; set hive.spark.client.connect.timeout=80000ms; ## 给的太少会报 spark client 初始化问题 ## 给的太多会 一直起不来 (driver 就起不来)，或者起来后 stage 一直不动（executor起不来） ## 多租户情况下考虑对应队列的上限的限制 (指定用户队列资源太少也会起不来) ## 下面的配置一般人物可以跑，参考 机器 16c 32g,混部。 yarn 的 配置 nodemanager.memory=12g ，scheluer.max=8g set spark.driver.memory=4g; set spark.executor.memory=8g; https://blog.csdn.net/benpaodexiaowoniu/article/details/105898501 "},"知识笔记/大数据组件/hive/hive_on_tez.html":{"url":"知识笔记/大数据组件/hive/hive_on_tez.html","title":"Hive On Tez","keywords":"","body":"https://my.oschina.net/shea1992/blog/3107972 参考文档 https://blog.csdn.net/m0_37813354/article/details/107849027 https://my.oschina.net/shea1992/blog/3107972 下载 protobuf-2.5.0.tar.gz // https://github.com/protocolbuffers/protobuf/releases tez-0.9.1源码 //http://www.apache.org/dyn/closer.lua/tez/0.9.1/ "},"知识笔记/大数据组件/hive/hive-Tips.html":{"url":"知识笔记/大数据组件/hive/hive-Tips.html","title":"Hive Tips","keywords":"","body":"hive-Tips [TOC] hive 网上说的很多知识是针对老版本的，包括很多看起来很新的博客，视频教程中提到的很多点，在新的版本中可能已经修复了这些不易用的点 2.0+ 的hive在很多已经优化了之前的问题。 常见的： 已经不是问题的问题 hive 两个表关联，小表放在前面。（已经优化了，放前放后都一样） 小表使用 hint 强制走 mapjoin。(默认对于什么是小表有个配置，在这个配置下的表会自动走mapjoin) 数据倾斜走 skewindata。 (这个hint 走起来大部分场景都会显著拖慢整个运行过程，因为它会首先进行全数据打散，这个通常无比的慢，然后处理，然后再聚合。通常比不加慢很多，如果有数据倾斜，大部分场景都可以代码调整逻辑来处理，使 skewindata 用建议测试有明显效果后使用，不要随便用，更不能每个都加) 搜索单条记录会扫描全表，导致很慢。（orc等列式存储出现后，orc本身的结构存在一定的类似索引的头文件信息，会根据数据值范围进行范围搜索，范围搜索单条记录，类比以前的思路比生成大临时表，然后处理这种思路会有比较新的变化） 压缩存储 直读压缩文件 https://www.jianshu.com/p/afd978026661 所有常见的压缩格式 包括 gz lzo bz2 snappy 对应hive都可以直接读取 zlib 待研究 传三个gz文本（逗号为英文，显示怪怪的要淡定） hive> dfs -ls /project/BDP/gztest ; Found 3 items -rw-rw-r-- 3 bdp BDP-ALL 80 2036-06-30 19:22 /project/BDP/gztest/txt1.txt.gz -rw-rw-r-- 3 bdp BDP-ALL 34 2036-06-30 19:22 /project/BDP/gztest/txt2.txt.gz -rw-rw-r-- 3 bdp BDP-ALL 33 2036-06-30 19:22 /project/BDP/gztest/txt3.txt.gz hive> dfs -text /project/BDP/gztest/* ; OpenSSH_6.6.1p1, OpenSSL 1.0.1e-fips 11 Feb 2013 ssssOpenSSH_6.6.1p1, OpenSSL 1.0.1e-fips 11 Feb 2013 2222 333 建表语句 直接建 txt create table mqtest_gz3 (col1 string ,col2 string ) row format delimited fields terminated by ',' location '/project/BDP/gztest/' ; 直接可以查 hive> select * from mqtest_gz3 ; hook status=true,operation=QUERY OK OpenSSH_6.6.1p1 OpenSSL 1.0.1e-fips 11 Feb 2013 ssssOpenSSH_6.6.1p1 OpenSSL 1.0.1e-fips 11 Feb 2013 2222 NULL 333 NULL 多字节分隔符 https://blog.csdn.net/u014307117/article/details/82428598 官方例子 CREATE TABLE test ( id string, hivearray array, hivemap map) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES (\"field.delim\"=\"[,]\",\"collection.delim\"=\":\",\"mapkey.delim\"=\"@\"); CREATE TABLE mqtest_mul ( col1 string , col2 string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES (\"field.delim\"=\"OpenSSL\",\"collection.delim\"=\":\",\"mapkey.delim\"=\"@\"); 小文件合并 新增合并步骤（Flie Merge） set hive.execution.engine=tez; set hive.merge.tezfiles=true; -- tez 引擎开启小文件合并 set hive.merge.mapredfiles=true; -- mr引擎开启小文件合并 set hive.merge.smallfiles.avgsize=128000000; set hive.merge.size.per.task=128000000; set hive.support.quoted.identifiers=none ; -- 开启正则表达式，下文中会用到 `(day)?+.+` 如果没有上述配置设置，生成的文件数与reduce 个数有关，默认在资源充足的情况下，会尽可能的起动很多个reduce,当前环境默认的单个任务的 reduce 上限为1024 。 所以在大部分情况下，每个hql任务的将生成1024个结果文件。 使用上述配置后，当结果文件小于128M 时回进行小文件合并。 hive> insert overwrite table inner_ods_bocs_inve partition (day='20350825') select `(day)?+.+` from inner_ods_bocs_inve where day='20350825' > ; Query ID = bdp_20360617150220_eab386e4-7eca-4113-90f7-7fb55e466463 Total jobs = 3 Launching Job 1 out of 3 Status: Running (Executing on YARN cluster with App id application_2094365163684_198629) ---------------------------------------------------------------------------------------------- VERTICES MODE STATUS TOTAL COMPLETED RUNNING PENDING FAILED KILLED ---------------------------------------------------------------------------------------------- Map 1 .......... container SUCCEEDED 2 2 0 0 0 0 VERTICES: 01/01 [==========================>>] 100% ELAPSED TIME: 9.87 s ---------------------------------------------------------------------------------------------- Stage-5 is filtered out by condition resolver. Stage-4 is selected by condition resolver. Stage-6 is filtered out by condition resolver. Launching Job 3 out of 3 ---------------------------------------------------------------------------------------------- VERTICES MODE STATUS TOTAL COMPLETED RUNNING PENDING FAILED KILLED ---------------------------------------------------------------------------------------------- File Merge ..... container SUCCEEDED 1 1 0 0 0 0 VERTICES: 01/01 [==========================>>] 100% ELAPSED TIME: 0.01 s ---------------------------------------------------------------------------------------------- Loading data to table bdp.inner_ods_bocs_inve partition (day=20350825) Loading data to table bdp.inner_ods_bocs_inve partition (day=20350825) Moved: 'hdfs://hdfsCluster/project/BDP/data/inner/ODS/01/INVE/20350825/000006_0' to trash at: hdfs://hdfsCluster/user/bdp/.Trash/Current .......此处有省略........ Moved: 'hdfs://hdfsCluster/project/BDP/data/inner/ODS/01/INVE/20350825/000044_0' to trash at: hdfs://hdfsCluster/user/bdp/.Trash/Current hook status=true,operation=QUERY OK Time taken: 12.718 seconds hive> dfs -ls /project/BDP/data/inner/ODS/01/INVE/20350825/ ; Found 1 items -rw-rw-r-- 3 bdp hdfs 502566 2036-06-17 15:02 /project/BDP/data/inner/ODS/01/INVE/20350825/000000_0 hive> 可以看到会新增一个File Merge 的stage 众多小文件会被合并成1个，具体文件个数由表实际文件大小和 上述配置参数决定。 除此之外还有一种官方的聚合手段 alter table inner_ods_bocs_inve partition (day=20350823) concatenate 可以使用上述方法对历史数据进行小文件合并，实测1024 第一次运行后生成 18个结果文件 多次执行后会越来越少，具体使用影响参数未知。 目前看 set mapreduce.input.fileinputformat.split.minsize=128000000 ; 这个参数时影响这个的，但是是有bug的，2018年提出目前仍然未解决。所以不建议使用 concatenate 做相关的精准控制。参考 https://issues.apache.org/jira/browse/HIVE-19090 hive 直接做归档 create table mqtest (col1 string ,col2 string ) partitioned by (day string ) ; insert into table mqtest partition (day=20190101) values ('3','2') ,('4','3') ; insert into table mqtest partition (day=20190101) values ('1','2') ,('2','3') ; create table mqtest_orc (col1 string ,col2 string ) partitioned by (day string ) ; insert into table mqtest_orc partition (day=20190101) values ('3','2') ,('4','3') ; insert into table mqtest_orc partition (day=20190101) values ('1','2') ,('2','3') ; set hive.archive.enabled=true; set hive.archive.har.parentdir.settable=true; set har.partfile.size=1099511627776; ALTER TABLE mqtest ARCHIVE PARTITION(day=20190101); ALTER TABLE mqtest_orc ARCHIVE PARTITION(day=20190101); insert into mqtest partition (day=20190102) values ('3','4') ; 执行结果 > ALTER TABLE mqtest ARCHIVE PARTITION(day=20190101); intermediate.archived is hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ARCHIVED intermediate.original is hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ORIGINAL Creating data.har for hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101 in hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101/.hive-staging_hive_2020-06-30_18-39-17_895_9088142084365566128-1/-ext-10000/partlevel Please wait... (this may take a while) 20/06/30 18:39:18 INFO client.RMProxy: Connecting to ResourceManager at hsnn01/22.188.12.56:8032 20/06/30 18:39:18 INFO client.RMProxy: Connecting to ResourceManager at hsnn01/22.188.12.56:8032 20/06/30 18:39:18 INFO client.RMProxy: Connecting to ResourceManager at hsnn01/22.188.12.56:8032 Moving hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101/.hive-staging_hive_2020-06-30_18-39-17_895_9088142084365566128-1/-ext-10000/partlevel to hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ARCHIVED Moving hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101 to hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ORIGINAL Moving hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ARCHIVED to hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101 OK Time taken: 23.844 seconds hive> select * from mqtest ; OK 3 2 20190101 4 3 20190101 1 2 20190101 2 3 20190101 Time taken: 0.245 seconds, Fetched: 4 row(s) hive> show create table mqtest ; OK CREATE TABLE `mqtest`( `col1` string, `col2` string) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest' TBLPROPERTIES ( 'transient_lastDdlTime'='1593513248') Time taken: 0.211 seconds, Fetched: 15 row(s) hive> dfs -ls /user/hive/warehouse/bdp.db/mqtest/ ; Found 1 items drwxr-xr-x - bdp hive 0 2020-06-30 18:39 /user/hive/warehouse/bdp.db/mqtest/day=20190101 hive> dfs -ls /user/hive/warehouse/bdp.db/mqtest/day=20190101 ; Found 1 items drwxr-xr-x - bdp hive 0 2020-06-30 18:39 /user/hive/warehouse/bdp.db/mqtest/day=20190101/data.har 同时存在 har 文件和普通文件时 select * 不受影响 har结果无法 insert overwrite 分区表 查看分区表命令 desc extended tb partition(dt=20170612) 分区表分区异常查询 MSCK REPAIR TABLE table_name; 常用知识 分区表每个分区相当于一个独立数据 对于只会使用基本sql的同事，需要充分掌握 where on 两个关键字在sql解析中的方式 对于常见的数据倾斜的场景请一定避免 对于分区表理解不是很充分的，建议使用粗暴的模式编写，使用子查询。 select * from aaa left join ( select * from bbb where bbb.day=20190101 ) bbb on aaa.col1=bbb.col1 where aaa.day='20190101' ---------------- select * from aaa left join bbb on aaa.col1=bbb.col1 where aaa.day='20190101' and bbb.day=20190101 ------------------ select * from aaa left join bbb on aaa.col1=bbb.col1 and bbb.day=20190101 where aaa.day='20190101' 上述sql等同于 select * from aaa left join bbb on aaa.col1=bbb.col1 and bbb.day='20190101' where aaa.day='20190101' 上面的写法是正确的，常见的错误写法 select * from aaa left join bbb on aaa.col1=bbb.col1 where aaa.day='20190101' and bbb.day='20190101' 这个就是没有分清 where 和 on 的基本逻辑，因为分区表是独立的元数据信息，目前hive2.0 解析器没有对这个逻辑进行自动修正。需要在写法上明确数据情况，进行规避。 PS: where 里写关联先关联，用关联结果再过滤 on里写关联，选出能关联的进行关联 上述情况也可以视作处理数据量太大，数据倾斜。 数据倾斜 拒绝使用 count(distinct) distinct 结果将会输出一个reduce,对于大量distinct 结果时，比如统计有多少种人名。 对于10亿级的人，count(distinct name) ,最终上千万种name 将会被一个map读取处理，发生数据倾斜。 单一map 读取千万条人名数据，引发倾斜。 简单避免可以写 tmp表，输出到表里分两个sql 来做。或拆分对应语句使用 group by去做。 尽可能不用full join 绝大部分的 full join 都可以使用 union group by 替换， full join 极易发生数据倾斜。需理解运行原理，并详细观察执行计划后使用。 多表关联的数据倾斜 对于有一种最常见的数据倾斜的场景请一定避免 select * from aaa left join bbb on aaa.col1=bbb.col1 left join ccc on bbb.col2=ccc.col2 上述是一个经典的数据倾斜。而且在传统数据库中并没有任何的问题。但在hive中基本一定会出现数据倾斜 这里弄不懂的话讲下基本原理 常见的面试中的什么是数据倾斜回答无外乎，某某数值太多，空值过高。 原理上默认的join操作时类似于hash 后打散，数据倾斜基本是由于关联键hash分布不均 上述经典场景中 通过 查看解析计划 explain xxxx 执行计划 是先处理 left join 假设 A 1000w 行，B 100w行，都能关联上 A * B 后 B.col2 字段只有100w行有数 使用B.col2 字段继续关联 C表900w 空在做hash 关联， 所以常见的场景下这种写法，一定会数据倾斜。 根据逻辑改成如下，用以规避相关倾斜问题，具体问题需要根据数据实际情况和执行计划情况进行分析 select * from aaa left join (select * from bbb left join ccc on bbb.col2=ccc.col2) tmp on aaa.col1=tmp.col1 map端倾斜 常见的map端倾斜只要由于计算引擎是按照container进行计算资源划分的，有默认值 默认值通常为128M 对于只有两列三列这种列数比较少的数据，如果是txt这种格式引入的，将会形成单个块处理超级多的条数的数据 对于这种数据如果处理sum(a+b)+sum(c+d),类似这种出现了500次-1000次时将会出现明显的map端倾斜 上述场景的经典场景是 类似hbase的标签数据，不同标签只有两列，客户号，是否、 如果要把他拉平成一个大宽表。 处理时如果使用 case when tag=sex , isvalid=1 then 1 else 0 , 这种逻辑500个标签重复500次，1000个标签重复1000次的话，将会直接引起map端倾斜，表现为map 长时间无法结束。 目前bdp采用的是比较蠢的解决方案，属于短时内对问题的应急处理，将对应1000个标签每50个拆分成1组，对这50个标签进行 case when 这种处理逻辑，每次降低轮扫数据后的处理逻辑复杂度。然后将200组结果进行join,200组的case when 在hive解析时将会进行分开处理，充分利用计算资源。 处理优化 5-6小时->40分钟-1小时 感觉比较好的方式是转成orc处理，待验证 通过配置拉大mapper资源这种方式，目前在hive中由于session级别安全限制，只有客户端模式可以提交，hiveserver模式无法修改默认影响mapper数量相关配置 其他 hive的元数据库是一个宝库。如有能力请尽可能多的了解。hive相关内容推荐lxw 的博客，很靠谱。 datax 3.0 可以支持直接传orc落表，后续bdp 可能会尝试使用替换主干代码，目前仅少量下发任务使用。datax 3.0 hdfsreader 读取orc有bug,一定确认相关版本bug是否修复。 https://www.jianshu.com/p/12c635abdd30 hive的字段长度定义无法按照 字符定义，使mysql 下发 varchar(10) 可能存10个汉字。对于utf-8 的hive需要定义成为 varchar(30) 。BDP目前对上游数据长度统一扩为3倍保证数据不丢失，租户使用时需知道相关内容。 hive中 txt格式下 varchar 性能明显由于 string。orc格式待验证，个人认为区别不大。 目前对于yarn中的队列资源是完全隔离的，类似与烟囱，并没有超分的机制。租户内对于任务资源使用分为 cpu紧张和内存紧张。运行时可以观察yarn 进行相关配置项调整。 建表格式 row format delimited fields terminated by '\\-128' stored as textfile; "},"知识笔记/大数据组件/hive/hive基础.html":{"url":"知识笔记/大数据组件/hive/hive基础.html","title":"hive基础","keywords":"","body":"启动 service mysql start systemctl mysql start systemctl mysql enabled nohup hive --service metastore & nohup hive --service hiveserver2 & hadoop start-all.sh 内部表与外部表的区别 hive默认存储内部表存储路径，不指定路径存储的话，路径为 hdfs 路径 /user/hive/warehouse/bdp.db/mqtest ; drop 会删除内部表 元数据 和 数据 外部表很多 命令不支持，比如 truncate 数据导入 txt 数据样式 vim ccc 1,2000000000000000000000000 2,2000000000000000000000000 hive默认分隔符 数据样式例子 AAAA\\001BBBB\\001CCCC1\\002CCCC2\\002CCCC3 实际shell 造数 ， echo AAAA$'\\001'BBBB$'\\001'CCCC1$'\\002'CCCC2$'\\002'CCCC3 gzip ccc ## hdfs dfs -put -f ccc.gz hdfs://bocfs/project/mqtest/ hdfs dfs -put -f ccc.gz //mqtest/ 建表 内部表 create table mqtest (col1 string ,col2 string ) location '/user/bdp/mqtest' 外部表 create external table mqtest_par (col1 decimal(13,8) ,col2 string ) partitioned by (day string) location '//mqtest stored as orc delimeter by ',' ; explain insert overwrite mqtest_par partition (day=20190101) values (1.1,\"ssss\"); explain insert into mq values ('1') ; insert overwrite mqtest_par partition (day) values (1.1,\"ssss\",20190101),(1.1,\"ssss\",20190120),(); 压缩格式 ZLIB gz bzip2 lzo Storage Format Description STORED AS TEXTFILE Stored as plain text files. TEXTFILE is the default file format, unless the configuration parameter hive.default.fileformat has a different setting.Use the DELIMITED clause to read delimited files.Enable escaping for the delimiter characters by using the 'ESCAPED BY' clause (such as ESCAPED BY '\\') Escaping is needed if you want to work with data that can contain these delimiter characters. A custom NULL format can also be specified using the 'NULL DEFINED AS' clause (default is '\\N'). STORED AS SEQUENCEFILE Stored as compressed Sequence File. STORED AS ORC Stored as ORC file format. Supports ACID Transactions & Cost-based Optimizer (CBO). Stores column-level metadata. STORED AS PARQUET Stored as Parquet format for the Parquet columnar storage format in Hive 0.13.0 and later; Use ROW FORMAT SERDE ... STORED AS INPUTFORMAT ... OUTPUTFORMAT syntax ... in Hive 0.10, 0.11, or 0.12. STORED AS AVRO Stored as Avro format in Hive 0.14.0 and later (see Avro SerDe). STORED AS RCFILE Stored as Record Columnar File format. STORED AS JSONFILE Stored as Json file format in Hive 4.0.0 and later. STORED BY Stored by a non-native table format. To create or link to a non-native table, for example a table backed by HBase or Druid or Accumulo. See StorageHandlers for more information on this option. INPUTFORMAT and OUTPUTFORMAT in the file_format to specify the name of a corresponding InputFormat and OutputFormat class as a string literal. For example, 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat'. For LZO compression, the values to use are 'INPUTFORMAT \"com.hadoop.mapred.DeprecatedLzoTextInputFormat\" OUTPUTFORMAT \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"' (see LZO Compression). CREATE TABLE `mq`( `col1` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs://localhost:9000/user/hive/warehouse/mq' 'hdfs://localhost:9000/user/hive/warehouse/kk.db/mq' TBLPROPERTIES ( 'transient_lastDdlTime'='1582077774') "},"知识笔记/大数据组件/hive/表结构导出.html":{"url":"知识笔记/大数据组件/hive/表结构导出.html","title":"表结构导出","keywords":"","body":"表结构导出 #!/usr/bin/env python #-*- coding:utf8 -*- # 从mysql中提取hive建表语句-指定的表 import os,sys import fileinput import datetime import mysql.connector reload(sys) sys.setdefaultencoding(\"utf8\") # 获取参数 if (len(sys.argv) == 3): Db_name = sys.argv[1].lower() Tab_Name = sys.argv[2].lower() else: print \"Parameters is illegal.\" print \"python export_tabname.py db tablename1,tablename2\" sys.exit(1) #IP host_valuse=\"xx.xx.xxx.xx\" #用户 user_valuse=\"hive\" #密码 passwd_valuse=\"xxxxxxxxxxx\" #数据库名称 database_valuse=\"hive\" #表名称 TabName = \"'\" + Tab_Name.replace(\",\",\"','\") + \"'\" def hive_create_table(host_valuse,user_valuse,passwd_valuse,database_valuse): conn = mysql.connector.connect(host=host_valuse,user=user_valuse,passwd=passwd_valuse,database=database_valuse,charset='utf8') mycursor = conn.cursor() # 获取DB_ID select_DB_ID = \"select DB_ID from DBS where NAME='{0}';\".format(Db_name) mycursor.execute(select_DB_ID) result_DB_ID = mycursor.fetchall() fo = open(\"create_tab.sql\", \"w\") for dir_DB_ID in result_DB_ID : # 获取数据库名 DB_ID = str(dir_DB_ID)[1:].split(',')[0] print(DB_ID) select_DB_NAME = \"select NAME from DBS where DB_ID=\"+DB_ID+\";\" print(select_DB_NAME ) mycursor.execute(select_DB_NAME) result_DB_NAME = mycursor.fetchone() fo.write(\"\\n===========数据库:\"+str(result_DB_NAME).split('\\'')[1]+\"===========\\n\") DBname=str(result_DB_NAME).split('\\'')[1] print '数据库名字：' + DBname print(result_DB_NAME) # 获取表名 select_table_name_sql = \"select TBL_NAME from TBLS where DB_ID='\"+ DB_ID+\"' and TBL_NAME in ({0})\".format(TabName) mycursor.execute(select_table_name_sql) result_table_names = mycursor.fetchall() for table_name in result_table_names : fo.write(\"\\nDROP TABLE IF EXISTS \"+DBname +'.`'+str(table_name).split('\\'')[1]+\"`;\") fo.write(\"\\nCREATE TABLE \"+DBname +'.`'+str(table_name).split('\\'')[1]+\"(\\n\") # 根据表名获取SD_ID select_table_SD_ID = \"select SD_ID from TBLS where tbl_name='\"+str(table_name).split('\\'')[1]+\"' and DB_ID=\"+DB_ID+\";\" print(select_table_SD_ID) mycursor.execute(select_table_SD_ID) result_SD_ID = mycursor.fetchone() print(result_SD_ID ) # 根据SD_ID获取CD_ID SD_ID=str(result_SD_ID)[1:].split(',')[0] select_table_CD_ID = \"select CD_ID from SDS where SD_ID=\"+str(result_SD_ID)[1:].split(',')[0]+\";\" print(select_table_CD_ID) mycursor.execute(select_table_CD_ID) result_CD_ID = mycursor.fetchone() print(result_CD_ID) # 根据CD_ID获取表的列 CD_ID=str(result_CD_ID)[1:].split(',')[0] select_table_COLUMN_NAME = \"select COLUMN_NAME,TYPE_NAME,COMMENT from COLUMNS_V2 where CD_ID=\"+str(result_CD_ID)[1:].split(',')[0]+\" order by INTEGER_IDX;\" print(select_table_COLUMN_NAME) mycursor.execute(select_table_COLUMN_NAME) result_COLUMN_NAME = mycursor.fetchall() print(result_COLUMN_NAME) index=0 for col,col_type,col_name in result_COLUMN_NAME: print(col) print(col_type) print(col_name) print(len(result_COLUMN_NAME) ) # 写入表的列和列的类型到文件 if col_name is None: fo.write(\" `\"+str(col)+\"` \"+str(col_type)) else: fo.write(\" `\"+str(col)+\"` \"+str(col_type) + \" COMMENT '\" + str(col_name) + \"'\") if index 0: #if result_PKEY_NAME_TYPE is not None: fo.write(\"\\nPARTITIONED BY (\\n\") #elif len(result_PKEY_NAME_TYPE) == 0: else : fo.write(\"\\n\") i=0 for pkey_name,pkey_type,PKEY_COMMENT in result_PKEY_NAME_TYPE: if str(PKEY_COMMENT) is None: fo.write(\" `\"+str(pkey_name)+\"` \"+str(pkey_type)) else: fo.write(\" `\"+str(pkey_name)+\"` \"+str(pkey_type) + \" COMMENT '\" + str(PKEY_COMMENT) + \"'\\n\") if i #!/usr/bin/env python #-*- coding:utf8 -*- # 从mysql中提取hive建表语句-指定数据库的所有表 import os,sys import fileinput import datetime import mysql.connector reload(sys) sys.setdefaultencoding(\"utf8\") # 获取参数 if (len(sys.argv) == 2): Db_name = sys.argv[1].lower() else: print \"Parameters is illegal.\" print \"python export_tabname.py db_name\" sys.exit(1) #IP host_valuse=\"xx.xx.xxx.xx\" #用户 user_valuse=\"hive\" #密码 passwd_valuse=\"xxxxxxxxx\" #数据库名称 database_valuse=\"hive\" def hive_create_table(host_valuse,user_valuse,passwd_valuse,database_valuse): conn = mysql.connector.connect(host=host_valuse,user=user_valuse,passwd=passwd_valuse,database=database_valuse,charset='utf8') mycursor = conn.cursor() # 获取DB_ID select_DB_ID = \"select DB_ID from DBS where NAME='{0}';\".format(Db_name) mycursor.execute(select_DB_ID) result_DB_ID = mycursor.fetchall() fo = open(\"create_tab.sql\", \"w\") for dir_DB_ID in result_DB_ID : # 获取数据库名 DB_ID = str(dir_DB_ID)[1:].split(',')[0] print(DB_ID) select_DB_NAME = \"select NAME from DBS where DB_ID=\"+DB_ID+\";\" print(select_DB_NAME ) mycursor.execute(select_DB_NAME) result_DB_NAME = mycursor.fetchone() fo.write(\"\\n===========数据库:\"+str(result_DB_NAME).split('\\'')[1]+\"===========\\n\") DBname=str(result_DB_NAME).split('\\'')[1] print '数据库名字：' + DBname print(result_DB_NAME) # 获取表名 select_table_name_sql = \"select TBL_NAME from TBLS where DB_ID='\"+ DB_ID+\"';\" mycursor.execute(select_table_name_sql) result_table_names = mycursor.fetchall() for table_name in result_table_names : fo.write(\"\\nDROP TABLE IF EXISTS \"+DBname +'.`'+str(table_name).split('\\'')[1]+\"`;\") fo.write(\"\\nCREATE TABLE IF NOT EXISTS \"+DBname +'.`'+str(table_name).split('\\'')[1]+\"`(\\n\") # 根据表名获取SD_ID select_table_SD_ID = \"select SD_ID from TBLS where tbl_name='\"+str(table_name).split('\\'')[1]+\"' and DB_ID=\"+DB_ID+\";\" print(select_table_SD_ID) mycursor.execute(select_table_SD_ID) result_SD_ID = mycursor.fetchone() print(result_SD_ID ) # 根据SD_ID获取CD_ID SD_ID=str(result_SD_ID)[1:].split(',')[0] select_table_CD_ID = \"select CD_ID from SDS where SD_ID=\"+str(result_SD_ID)[1:].split(',')[0]+\";\" print(select_table_CD_ID) mycursor.execute(select_table_CD_ID) result_CD_ID = mycursor.fetchone() print(result_CD_ID) # 根据CD_ID获取表的列 CD_ID=str(result_CD_ID)[1:].split(',')[0] select_table_COLUMN_NAME = \"select COLUMN_NAME,TYPE_NAME,COMMENT from COLUMNS_V2 where CD_ID=\"+str(result_CD_ID)[1:].split(',')[0]+\" order by INTEGER_IDX;\" print(select_table_COLUMN_NAME) mycursor.execute(select_table_COLUMN_NAME) result_COLUMN_NAME = mycursor.fetchall() print(result_COLUMN_NAME) index=0 for col,col_type,col_name in result_COLUMN_NAME: print(col) print(col_type) print(col_name) print(len(result_COLUMN_NAME) ) # 写入表的列和列的类型到文件 if col_name is None: fo.write(\" `\"+str(col)+\"` \"+str(col_type)) else: fo.write(\" `\"+str(col)+\"` \"+str(col_type) + \" COMMENT '\" + str(col_name) + \"'\") if index 0: #if result_PKEY_NAME_TYPE is not None: fo.write(\"\\nPARTITIONED BY (\\n\") #elif len(result_PKEY_NAME_TYPE) == 0: else : fo.write(\"\\n\") i=0 for pkey_name,pkey_type,PKEY_COMMENT in result_PKEY_NAME_TYPE: if str(PKEY_COMMENT) is None: fo.write(\" `\"+str(pkey_name)+\"` \"+str(pkey_type)) else: fo.write(\" `\"+str(pkey_name)+\"` \"+str(pkey_type) + \" COMMENT '\" + str(PKEY_COMMENT) + \"'\\n\") if i "},"知识笔记/大数据组件/任务调度/etljc/etljc.html":{"url":"知识笔记/大数据组件/任务调度/etljc/etljc.html","title":"Etljc","keywords":"","body":"etljc 初始化库 建表 source etljc.sql 初始化 sys_cfg 初始化 sys_router 软链接 ## 连走整个目录 ## sudo sh -c 'mkdip -p /data/bdp ;ln -s /data/bdp /bdp ;chown bdp:bdp /data/bdp ;chmod 777 /bdp ;' ## 连走 etljc sudo sh -c 'mkdir -p /data/bdp/bin/etljc /data/bdp/log/etljc ; ln -s /data/bdp/bin/etljc /bdp/bin/etljc ; ln -s /data/bdp/log/etljc /bdp/log/etljc ; chmod 777 /bdp ; chown bdp:bdp -R /data/bdp /bdp/bin/etljc /bdp/log/etljc' 介质传输 使用bdp用户，依赖先创建工作空间 scp -r root@10.182.212.81:/data/setup/etljc*/* /bdp/bin/etljc/ chmod 755 -R /bdp/bin/etljc/ 修改配置 加hosts etljcmysql xx.xx.xx.xx 10.182.212.81 hsnn01 10.182.212.82 hsnn02 10.182.212.83 hsdn01 10.182.212.84 hsdn02 10.182.212.85 hsdn03 10.182.212.86 hshv01 10.182.212.87 hshv02 10.182.212.88 hsmsg01 10.182.212.89 hsmsg02 10.182.212.90 hsmsg03 10.182.212.91 hssp01 10.182.212.92 hssp02 10.182.212.93 hssp03 10.182.212.94 hszk01 10.182.212.95 hszk02 10.182.212.96 hszk03 10.182.212.97 hsdx01 hivemeta 10.182.212.98 hsjb01 10.182.212.99 hsfp01 10.182.212.100 hsdb01 etljcmysql "},"知识笔记/大数据组件/存储/hdfs/hdfs.html":{"url":"知识笔记/大数据组件/存储/hdfs/hdfs.html","title":"Hdfs","keywords":"","body":"hdfs [toc] 压缩 https://blog.csdn.net/weixin_33905756/article/details/91394345 txt -- zlib orc -- zlib python zlib 小文件问题 https://blog.csdn.net/weixin_42582592/article/details/85084575 har 官方文档 https://hadoop.apache.org/docs/r3.1.3/hadoop-archives/HadoopArchives.html 生成 ## har包名字 压哪 用几个副本 输出到哪，默认hdfs上 hadoop archive -archiveName zoo.har -p /foo/bar [dir1 dir2 dir3 ] -r 3 /outputdir 生成结果 原始文件不会清除，上面的命令是新生成一个har，为了节约namenode要清除掉以前的。 查看 hdfs dfs -ls har:///project/ccc.har 其实就是路径 前缀要用har,紧跟hdfs目录 hive 直接做归档 create table mqtest (col1 string ,col2 string ) partitioned by (day string ) ; insert into table mqtest partition (day=20190101) values ('3','2') ,('4','3') ; insert into table mqtest partition (day=20190101) values ('1','2') ,('2','3') ; create table mqtest_orc (col1 string ,col2 string ) partitioned by (day string ) ; insert into table mqtest_orc partition (day=20190101) values ('3','2') ,('4','3') ; insert into table mqtest_orc partition (day=20190101) values ('1','2') ,('2','3') ; set hive.archive.enabled=true; set hive.archive.har.parentdir.settable=true; set har.partfile.size=1099511627776; ALTER TABLE mqtest ARCHIVE PARTITION(day=20190101); ALTER TABLE mqtest_orc ARCHIVE PARTITION(day=20190101); insert into mqtest partition (day=20190102) values ('3','4') ; 执行结果 > ALTER TABLE mqtest ARCHIVE PARTITION(day=20190101); intermediate.archived is hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ARCHIVED intermediate.original is hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ORIGINAL Creating data.har for hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101 in hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101/.hive-staging_hive_2020-06-30_18-39-17_895_9088142084365566128-1/-ext-10000/partlevel Please wait... (this may take a while) 20/06/30 18:39:18 INFO client.RMProxy: Connecting to ResourceManager at hsnn01/22.188.12.56:8032 20/06/30 18:39:18 INFO client.RMProxy: Connecting to ResourceManager at hsnn01/22.188.12.56:8032 20/06/30 18:39:18 INFO client.RMProxy: Connecting to ResourceManager at hsnn01/22.188.12.56:8032 Moving hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101/.hive-staging_hive_2020-06-30_18-39-17_895_9088142084365566128-1/-ext-10000/partlevel to hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ARCHIVED Moving hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101 to hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ORIGINAL Moving hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101_INTERMEDIATE_ARCHIVED to hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest/day=20190101 OK Time taken: 23.844 seconds hive> select * from mqtest ; OK 3 2 20190101 4 3 20190101 1 2 20190101 2 3 20190101 Time taken: 0.245 seconds, Fetched: 4 row(s) hive> show create table mqtest ; OK CREATE TABLE `mqtest`( `col1` string, `col2` string) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs://hsnn01:8020/user/hive/warehouse/bdp.db/mqtest' TBLPROPERTIES ( 'transient_lastDdlTime'='1593513248') Time taken: 0.211 seconds, Fetched: 15 row(s) hive> dfs -ls /user/hive/warehouse/bdp.db/mqtest/ ; Found 1 items drwxr-xr-x - bdp hive 0 2020-06-30 18:39 /user/hive/warehouse/bdp.db/mqtest/day=20190101 hive> dfs -ls /user/hive/warehouse/bdp.db/mqtest/day=20190101 ; Found 1 items drwxr-xr-x - bdp hive 0 2020-06-30 18:39 /user/hive/warehouse/bdp.db/mqtest/day=20190101/data.har 同时存在 har 文件和普通文件时 select * 不受影响 text https://sukbeta.github.io/hadoop-lzo-gz-bz2/ text 命令可以直接查看对应压缩包，支持格式可以看上面的文档 hdfs -text har:///project/ccc.har/ccc.gz 刷新角色组（可能没什么用） su - hdfs -s /bin/bash -c \"hdfs dfsadmin -refreshUserToGroupsMappings\" fsck 丢失block su - hdfs -s /bin/bash -c \"hdfs fsck /\" su - hdfs -s /bin/bash -c \"hdfs debug recoverLease -path /hbase/oldWALs/pv2-00000000000000004078.log \" "},"知识笔记/大数据组件/存储/hdfs/cdh-hdfs开启simple权限认证.html":{"url":"知识笔记/大数据组件/存储/hdfs/cdh-hdfs开启simple权限认证.html","title":"cdh-hdfs开启simple权限认证","keywords":"","body":"开启权限认证 http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html "},"知识笔记/大数据组件/存储/hdfs/nn QJM问题.html":{"url":"知识笔记/大数据组件/存储/hdfs/nn QJM问题.html","title":"nn QJM问题","keywords":"","body":"2018-12-24 22:45:30,418 WARN client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 19032 ms (timeout=20000 ms) for a response for sendE dits. Succeeded so far: [10.10.22.3:8485]. Exceptions so far: [10.10.22.2:8485: Journal disabled until next roll] 2018-12-24 22:45:31,688 FATAL namenode.FSEditLog (JournalSet.java:mapJournalsAndReportErrors(398)) - Error: flush failed for required journal (Journal AndStream(mgr=QJM to [10.10.22.3:8485, 10.10.22.2:8485, 10.10.22.4:8485], stream=QuorumOutputStream starting at txid 35387466)) java.io.IOException: Timed out waiting 20000ms for a quorum of nodes to respond. 改这个 dfs.qjournal.write-txns.timeout.ms 60000 "},"知识笔记/大数据组件/存储/hdfs/orc.html":{"url":"知识笔记/大数据组件/存储/hdfs/orc.html","title":"Orc","keywords":"","body":"orc 文件问题 CDH 下 /data/ hadooplib hive/lib java 下 "},"知识笔记/大数据组件/搜索/es/elasticsearch.html":{"url":"知识笔记/大数据组件/搜索/es/elasticsearch.html","title":"Elasticsearch","keywords":"","body":"Elasticsearch CDH 继承 es 管理 参考文档 https://blog.csdn.net/guoliduo/article/details/105072857 https://blog.csdn.net/weixin_39039055/article/details/113769701 问题分析： ## jdk问题 ln -s /data/app/jdk /usr/java/latest ## 启动权限问题 chmod 777 -R /opt/cloudera/parcels/ELASTICSEARCH-0.0.5.elasticsearch.p0.5 ## ## chown elasticsearch:elasticsearch -R /opt/cloudera/parcels/ELASTICSEARCH-0.0.5.elasticsearch.p0.5/ 配置集群添加主节点配置 重点关注 参考文档2 中包含 错误2：使用时候报错，如http://cdh3:9200/_cat/nodes?pretty 时候报错 master_not_discovered_exception 解决：修改ES配置 增加masternodes 配置：cluster.initial_master_nodes: [“cdh3”] CDH启动对应服务即可 Kibana "},"知识笔记/大数据组件/数据安全与权限管理/kerberos.html":{"url":"知识笔记/大数据组件/数据安全与权限管理/kerberos.html","title":"Kerberos","keywords":"","body":""},"知识笔记/大数据组件/数据安全与权限管理/ldap.html":{"url":"知识笔记/大数据组件/数据安全与权限管理/ldap.html","title":"Ldap","keywords":"","body":""},"知识笔记/大数据组件/数据安全与权限管理/多租户.html":{"url":"知识笔记/大数据组件/数据安全与权限管理/多租户.html","title":"多租户","keywords":"","body":"ranger编译和安装 腾讯课堂 视频（1.2.0） https://ke.qq.com/webcourse/index.html#cid=443147&term_id=100529499&taid=3859066770604811&type=1024&vid=5285890793895524135 下载 下载 ranger 目前版本 2.0.0 下载渠道， https://dist.apache.org/repos/dist/release/ranger/2.0.0/ 下了release 稳定版，没有像大部分人从 gitlhub 直接下载源码。 编译 准备工作 cd /d/wsl/setup tar -xvf apache-ranger-2.0.0.tar.gz ## 直接把名字改了，有时间建议建软连接 mv apache-ranger-2.0.0 ranger 查看 我有maven mq5445@DESKTOP-7IL9SC5:/d/wsl/setup/ranger$ mvn --version Apache Maven 3.6.0 Maven home: /usr/share/maven Java version: 1.8.0_241, vendor: Oracle Corporation, runtime: /mnt/d/wsl/setup/java/jre Default locale: en, platform encoding: UTF-8 OS name: \"linux\", version: \"4.4.0-18362-microsoft\", arch: \"amd64\", family: \"unix\" 根据cdh 版本改 pom https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_63_packaging.html CDH 6.3.2 Component Component Version Apache Avro 1.8.2 Apache Flume 1.9.0 Apache Hadoop 3.0.0 Apache HBase 2.1.4 HBase Indexer 1.5 Apache Hive 2.1.1 Hue 4.4.0 Apache Impala 3.2.0 Apache Kafka 2.2.1 Kite SDK 1.0.0 Apache Kudu 1.10.0 Apache Solr 7.4.0 Apache Oozie 5.1.0 Apache Parquet 1.9.0 Parquet-format 2.4.0 Apache Pig 0.17.0 Apache Sentry 2.1.0 Apache Spark 2.4.0 Apache Sqoop 1.4.7 Apache ZooKeeper 3.4.5 对应修改 pom 我改了以下这些 3.0.0 2.1.1 2.2.1 3.4.5 7.0.105 hbase 的版本有问题，改下面的会报错。 所以没改保留了 hbase 2.0.2去编译，也有人和我一样这里会报错, 下面的不要按cdh的改。会有问题 2.1.4 2.1.4 2.1.4 2.1.4 1.4.7 编译 开始编译 mvn clean compile package assembly:assembly -Dpmd.skip=true install -DskipTests -Drat.skip=true 这里 跳过了 pmd 静态代码检测。加上会报错。特别蛋疼。搞了2天这个。 看有的人说要装 git 和 python 我的都有 python 版本 mq5445@DESKTOP-7IL9SC5:~$ python --version Python 2.7.17 Downloaded from alimaven: http://maven.aliyun.com/nexus/content/groups/public/net/minidev/json-smart/maven-metadata.xml (850 B at 435 B/s) 这里补充添加了 osc 的源，ali 的似乎没有jboss sudo vim /etc/maven/settings.xml 找到 mirrors位置里添加 osc * http://maven.oschina.net/content/groups/public/ hbase 版本报错。看有的老哥也这报错。 查了下 hive-agent 报错，默认是3.1.2 改2.1.1 有问题 查了下 ranger 0.7 版本才支持2.1.1 这个版本。 cdh 的这个hive版本太老了 编译完了。用不起来。 大佬的做法。 https://blog.csdn.net/tomalun/article/details/105490214 下了个 ranger 1.2.0 把对应的 hive-agent 覆盖掉 ranger 2.0.0 的 hive-agent 第10次编译后 [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] ranger 2.0.0 ....................................... SUCCESS [ 0.714 s] [INFO] Jdbc SQL Connector 2.0.0 ........................... SUCCESS [ 0.362 s] [INFO] Credential Support 2.0.0 ........................... SUCCESS [ 1.299 s] [INFO] Audit Component 2.0.0 .............................. SUCCESS [ 1.586 s] [INFO] Common library for Plugins 2.0.0 ................... SUCCESS [ 2.552 s] [INFO] Installer Support Component 2.0.0 .................. SUCCESS [ 0.253 s] [INFO] Credential Builder 2.0.0 ........................... SUCCESS [ 1.004 s] [INFO] Embedded Web Server Invoker 2.0.0 .................. SUCCESS [ 1.253 s] [INFO] Key Management Service 2.0.0 ....................... SUCCESS [ 1.781 s] [INFO] ranger-plugin-classloader 2.0.0 .................... SUCCESS [ 0.310 s] [INFO] HBase Security Plugin Shim 2.0.0 ................... SUCCESS [ 2.004 s] [INFO] HBase Security Plugin 2.0.0 ........................ SUCCESS [ 2.483 s] [INFO] Hdfs Security Plugin 2.0.0 ......................... SUCCESS [ 1.517 s] [INFO] Hive Security Plugin 1.2.0 ......................... SUCCESS [ 2.599 s] [INFO] Knox Security Plugin Shim 2.0.0 .................... SUCCESS [ 1.584 s] [INFO] Knox Security Plugin 2.0.0 ......................... SUCCESS [ 2.815 s] [INFO] Storm Security Plugin 2.0.0 ........................ SUCCESS [ 1.566 s] [INFO] YARN Security Plugin 2.0.0 ......................... SUCCESS [ 1.439 s] [INFO] Ozone Security Plugin 2.0.0 ........................ SUCCESS [ 1.794 s] [INFO] Ranger Util 2.0.0 .................................. SUCCESS [ 3.815 s] [INFO] Unix Authentication Client 2.0.0 ................... SUCCESS [ 0.760 s] [INFO] Security Admin Web Application 2.0.0 ............... SUCCESS [01:05 min] [INFO] KAFKA Security Plugin 2.0.0 ........................ SUCCESS [ 1.394 s] [INFO] SOLR Security Plugin 2.0.0 ......................... SUCCESS [ 3.854 s] [INFO] NiFi Security Plugin 2.0.0 ......................... SUCCESS [ 1.214 s] [INFO] NiFi Registry Security Plugin 2.0.0 ................ SUCCESS [ 1.188 s] [INFO] Unix User Group Synchronizer 2.0.0 ................. SUCCESS [ 1.736 s] [INFO] Ldap Config Check Tool 2.0.0 ....................... SUCCESS [ 0.456 s] [INFO] Unix Authentication Service 2.0.0 .................. SUCCESS [ 1.300 s] [INFO] KMS Security Plugin 2.0.0 .......................... SUCCESS [ 1.463 s] [INFO] Tag Synchronizer 2.0.0 ............................. SUCCESS [ 1.631 s] [INFO] Hdfs Security Plugin Shim 2.0.0 .................... SUCCESS [ 1.128 s] [INFO] Hive Security Plugin Shim 2.0.0 .................... SUCCESS [ 3.083 s] [INFO] YARN Security Plugin Shim 2.0.0 .................... SUCCESS [ 1.496 s] [INFO] OZONE Security Plugin Shim 2.0.0 ................... SUCCESS [ 1.766 s] [INFO] Storm Security Plugin shim 2.0.0 ................... SUCCESS [ 1.422 s] [INFO] KAFKA Security Plugin Shim 2.0.0 ................... SUCCESS [ 1.164 s] [INFO] SOLR Security Plugin Shim 2.0.0 .................... SUCCESS [ 1.994 s] [INFO] Atlas Security Plugin Shim 2.0.0 ................... SUCCESS [ 1.349 s] [INFO] KMS Security Plugin Shim 2.0.0 ..................... SUCCESS [ 1.464 s] [INFO] ranger-examples 2.0.0 .............................. SUCCESS [ 0.113 s] [INFO] Ranger Examples - Conditions and ContextEnrichers 2.0.0 SUCCESS [ 1.155 s] [INFO] Ranger Examples - SampleApp 2.0.0 .................. SUCCESS [ 0.264 s] [INFO] Ranger Examples - Ranger Plugin for SampleApp 2.0.0 SUCCESS [ 1.239 s] [INFO] Ranger Tools 2.0.0 ................................. SUCCESS [ 1.179 s] [INFO] Atlas Security Plugin 2.0.0 ........................ SUCCESS [ 1.484 s] [INFO] Sqoop Security Plugin 2.0.0 ........................ SUCCESS [ 1.313 s] [INFO] Sqoop Security Plugin Shim 2.0.0 ................... SUCCESS [ 1.196 s] [INFO] Kylin Security Plugin 2.0.0 ........................ SUCCESS [ 1.507 s] [INFO] Kylin Security Plugin Shim 2.0.0 ................... SUCCESS [ 1.401 s] [INFO] Elasticsearch Security Plugin Shim 2.0.0 ........... SUCCESS [ 0.454 s] [INFO] Elasticsearch Security Plugin 2.0.0 ................ SUCCESS [ 1.295 s] [INFO] Presto Security Plugin 2.0.0 ....................... SUCCESS [ 1.310 s] [INFO] Presto Security Plugin Shim 2.0.0 .................. SUCCESS [ 1.307 s] [INFO] Unix Native Authenticator 2.0.0 .................... SUCCESS [ 0.695 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 42:43 min [INFO] Finished at: 2020-07-22T10:12:59+08:00 [INFO] ------------------------------------------------------------------------ 结果： root@DESKTOP-7IL9SC5:/mnt/d/wsl/setup/ranger# ll target/ total 1534276 drwxr-xr-x 1 root root 4096 Jul 22 10:10 ./ drwxrwxr-x 1 mq5445 mq5445 4096 Jul 22 09:30 ../ -rw-r--r-- 1 root root 30 Jul 22 10:10 .plxarc drwxr-xr-x 1 root root 4096 Jul 22 09:30 antrun/ drwxr-xr-x 1 root root 4096 Jul 22 10:09 archive-tmp/ drwxr-xr-x 1 root root 4096 Jul 22 09:30 maven-shared-archive-resources/ -rw-r--r-- 1 root root 215913533 Jul 22 10:01 ranger-2.0.0-admin.tar.gz -rw-r--r-- 1 root root 217022494 Jul 22 10:03 ranger-2.0.0-admin.zip -rw-r--r-- 1 root root 27540587 Jul 22 10:06 ranger-2.0.0-atlas-plugin.tar.gz -rw-r--r-- 1 root root 27570212 Jul 22 10:06 ranger-2.0.0-atlas-plugin.zip -rw-r--r-- 1 root root 30983732 Jul 22 10:09 ranger-2.0.0-elasticsearch-plugin.tar.gz -rw-r--r-- 1 root root 31019906 Jul 22 10:09 ranger-2.0.0-elasticsearch-plugin.zip -rw-r--r-- 1 root root 26575282 Jul 22 09:53 ranger-2.0.0-hbase-plugin.tar.gz -rw-r--r-- 1 root root 26594864 Jul 22 09:53 ranger-2.0.0-hbase-plugin.zip -rw-r--r-- 1 root root 23709141 Jul 22 09:51 ranger-2.0.0-hdfs-plugin.tar.gz -rw-r--r-- 1 root root 23735788 Jul 22 09:51 ranger-2.0.0-hdfs-plugin.zip -rw-r--r-- 1 root root 23745317 Jul 22 09:52 ranger-2.0.0-hive-plugin.tar.gz -rw-r--r-- 1 root root 23769378 Jul 22 09:53 ranger-2.0.0-hive-plugin.zip -rw-r--r-- 1 root root 39811408 Jul 22 09:55 ranger-2.0.0-kafka-plugin.tar.gz -rw-r--r-- 1 root root 39847327 Jul 22 09:56 ranger-2.0.0-kafka-plugin.zip -rw-r--r-- 1 root root 90070439 Jul 22 10:04 ranger-2.0.0-kms.tar.gz -rw-r--r-- 1 root root 90175690 Jul 22 10:05 ranger-2.0.0-kms.zip -rw-r--r-- 1 root root 28272440 Jul 22 09:54 ranger-2.0.0-knox-plugin.tar.gz -rw-r--r-- 1 root root 28295654 Jul 22 09:54 ranger-2.0.0-knox-plugin.zip -rw-r--r-- 1 root root 23684723 Jul 22 10:08 ranger-2.0.0-kylin-plugin.tar.gz -rw-r--r-- 1 root root 23718242 Jul 22 10:08 ranger-2.0.0-kylin-plugin.zip -rw-r--r-- 1 root root 34256 Jul 22 10:04 ranger-2.0.0-migration-util.tar.gz -rw-r--r-- 1 root root 37740 Jul 22 10:04 ranger-2.0.0-migration-util.zip -rw-r--r-- 1 root root 26126862 Jul 22 09:57 ranger-2.0.0-ozone-plugin.tar.gz -rw-r--r-- 1 root root 26159348 Jul 22 09:57 ranger-2.0.0-ozone-plugin.zip -rw-r--r-- 1 root root 39654366 Jul 22 10:10 ranger-2.0.0-presto-plugin.tar.gz -rw-r--r-- 1 root root 39691150 Jul 22 10:10 ranger-2.0.0-presto-plugin.zip -rw-r--r-- 1 root root 22146467 Jul 22 10:05 ranger-2.0.0-ranger-tools.tar.gz -rw-r--r-- 1 root root 22156287 Jul 22 10:05 ranger-2.0.0-ranger-tools.zip -rw-r--r-- 1 root root 26905522 Jul 22 09:58 ranger-2.0.0-solr-plugin.tar.gz -rw-r--r-- 1 root root 26938293 Jul 22 09:58 ranger-2.0.0-solr-plugin.zip -rw-r--r-- 1 root root 42230 Jul 22 10:03 ranger-2.0.0-solr_audit_conf.tar.gz -rw-r--r-- 1 root root 45636 Jul 22 10:03 ranger-2.0.0-solr_audit_conf.zip -rw-r--r-- 1 root root 23695951 Jul 22 10:07 ranger-2.0.0-sqoop-plugin.tar.gz -rw-r--r-- 1 root root 23724222 Jul 22 10:07 ranger-2.0.0-sqoop-plugin.zip -rw-r--r-- 1 root root 4071859 Jul 22 10:06 ranger-2.0.0-src.tar.gz -rw-r--r-- 1 root root 6340193 Jul 22 10:06 ranger-2.0.0-src.zip -rw-r--r-- 1 root root 36938489 Jul 22 09:54 ranger-2.0.0-storm-plugin.tar.gz -rw-r--r-- 1 root root 36963193 Jul 22 09:55 ranger-2.0.0-storm-plugin.zip -rw-r--r-- 1 root root 33709608 Jul 22 10:04 ranger-2.0.0-tagsync.tar.gz -rw-r--r-- 1 root root 33720483 Jul 22 10:04 ranger-2.0.0-tagsync.zip -rw-r--r-- 1 root root 16191997 Jul 22 10:03 ranger-2.0.0-usersync.tar.gz -rw-r--r-- 1 root root 16208427 Jul 22 10:04 ranger-2.0.0-usersync.zip -rw-r--r-- 1 root root 23701001 Jul 22 09:56 ranger-2.0.0-yarn-plugin.tar.gz -rw-r--r-- 1 root root 23730557 Jul 22 09:57 ranger-2.0.0-yarn-plugin.zip -rw-r--r-- 1 root root 5 Jul 22 10:10 version 安装Ranger控制台：Ranger-admin 这里又折腾了几个小时，连mysql一直在报错 tar -zxvf ranger-2.0.0-admin.tar.gz vim install.properties 设置了一大堆 连mysql 的 db_root_user=root db_root_password=Bdp@boc01! db_host=22.188.12.73 db_name=ranger db_user=ranger db_password=Bdp@boc01! rangerAdmin_password=Bdp@boc01! rangerTagsync_password=Bdp@boc01! rangerUsersync_password=Bdp@boc01! keyadmin_password=Bdp@boc01! #audit_store=db 先没开审计 我自己感觉用处不大 audit_db_name=ranger_audit audit_db_user=ranger_audit audit_db_password=Bdp@boc01! policymgr_http_enabled=true policymgr_https_keystore_file= policymgr_https_keystore_keyalias=rangeradmin policymgr_https_keystore_password=Bdp@boc01! 跑起来一直在报如下错误 2020-07-15 17:26:19,354 [JISQL] /data/app/jdk11/jdk-11.0.5/bin/java -cp /data/ranger/lib/mysql-connector-java-5.1.38.jar:/data/ranger/target/ranger-2.0.0-admin/jisql/lib/* org.apache.util.sql.Jisql -driver mysqlconj -cstring jdbc:mysql://22.188.12.73/ranger -u 'ranger' -p '********' -noheader -trim -c \\; -query \"select 1;\" Wed Jul 15 17:26:20 CST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. SQLException : SQL state: 42000 com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Access denied for user 'ranger'@'22.188.12.73' to database 'ranger' ErrorCode: 1044 2020-07-15 17:26:20,524 [E] Can't establish connection!! Exiting.. 2020-07-15 17:26:20,524 [I] Please run DB setup first or contact Administrator.. 2020-07-15 17:26:20,540 [E] 'setup_mode' not found in /data/ranger/target/ranger-2.0.0-admin/install.properties file while getting....!! 报错大概意思是没有权限 root 登进去，发现root 本身没有本地登陆权限 没有 /tmp/mysql.sock 查看 /etc/my.cnf 新增对应命令指向对应mysqld 配置的sock [clinet] sock=/ccc/mysql.sock [mysql] sock=/ccc/mysql.sock 改完发现 root 本地可以进去了，但是还是报上面的错误 看了下报错的位置似乎有区别。网上找了下，是root -h 指定对应ip是 没有grant 权限 use mysql ; select * from user where user='root' ; update user set grant_priv='Y' where User='root' ; FLUSH PRIVILEGES; 初始化 加启动web ./setup.sh ews/ranger-admin-services.sh start 启动后可以看到 6080 端口的web端 安装ranger Usersync https://blog.csdn.net/andyguan01_2/article/details/88950551 需要使用root用户 Ranger Usersync会将linux系统用户同步到ranger数据库。 6.1 修改配置文件install.properties cd /u01/app/ranger-2.0.0/ranger-2.0.0-SNAPSHOT-usersync vi install.properties 12 修改的参数如下： POLICY_MGR_URL=http://10.200.4.117:6080 SYNC_SOURCE=unix #多久同步一次数据，单位为分钟 SYNC_INTERVAL=1 logdir=/data/ranger/logs/usersync 12345 6.2 执行初始化设置 执行.： /setup.sh 1 6.3 启动Ranger Usersync 执行： ./ranger-usersync-services.sh start 1 ranger-usersync-services.sh的参数如下： [root@oracle02 ranger-2.0.0-SNAPSHOT-usersync]# ./ranger-usersync-services.sh Invalid argument []; Usage: Only start | stop | restart | version, are supported. 123 验证是否安装成功：打开Ranger控制台页面，点击“Settings->Users/Groups”，可以看到从Linux同步过来的用户信息则说明成功。如下： 装hive插件 在hive-server2 节点安装hive-plugin 参考文档 （ps:大佬很奔放，格式一团糊 🤮） https://blog.csdn.net/huangyq5240/article/details/102721679 直接粘的大佬的，目录 6.3.2 对应要换，主要就是 把 ranger-hive-plugin的包放到 cdh 里去 ## 放到对应的cdh目录下去 cp /home/xxxxxxx/ranger-0.5.4-SNAPSHOT-hive-plugin.tar.gz /opt/cloudera/parcels/CDH/lib ## 软连接或改名字 ln -s ranger-0.5.4-SNAPSHOT-hive-plugin ranger-hive-plugin ## 改配置 cd ranger-hive-plugin/ vim install.properties #对应找到更改 ## 必须改 POLICY_MGR_URL=http://172.17.8.94:6080 REPOSITORY_NAME=hivedev ## 审计（我没改） XAAUDIT.SOLR.ENABLE=true XAAUDIT.SOLR.URL=http://172.17.8.4:6083/solr/ranger_audits XAAUDIT.SOLR.FILE_SPOOL_DIR=/var/log/hive/audit/solr/spool ## 改 CUSTOM_USER=hive CUSTOM_GROUP=hive 启动，最终其实是 要重启 hiveserver2 才会生效，所以这里我觉得只是东西分发了下 #启动插件，感觉其实就是 把东西一顿复制，没具体看 ./enable-hive-plugin.sh ## /opt/cloudera/parcels/CDH/lib/ranger-hive-plugin/enable-hive-plugin.sh 改cm-agent 对应的分发程序，我感觉是 保证ranger生成的三个配置文件可以同步到对应的 impala ,hivemeta, hiveserver2 等服务上去 find / -name \"cloudera-config.sh\" ## 找个合适的，对应的位置 ## 大佬的位置 sudo vim /usr/lib64/cmf/service/common/cloudera-config.sh vim /opt/cloudera/cm-agent/service/common/cloudera-config.sh # When created, the final resting place of config files is unknown, # so it is marked as . We know what this directory will # be here in this script, so search-replace it. replace_conf_dir() { find $CONF_DIR -type f ! -path \"$CONF_DIR/logs/*\" ! -name \"*.log\" ! -name \"*.keytab\" ! -name \"*jceks\" -exec perl -pi -e \"s##$CONF_DIR#g\" {} \\; ### 以下为新增,把对应的ranger 的配置同步到 主配置, hive [[ -e /etc/hive/conf/ranger-hive-audit.xml ]] && { scp /etc/hive/conf/ranger-hive-audit.xml $CONF_DIR/ scp /etc/hive/conf/ranger-hive-security.xml $CONF_DIR/ scp /etc/hive/conf/ranger-policymgr-ssl.xml $CONF_DIR/ ; } ### 以下是 yarn 的 [[ -e /etc/hadoop/conf/ranger-yarn-audit.xml ]] && { scp /etc/hadoop/conf/ranger-policymgr-ssl.xml $CONF_DIR/ scp /etc/hadoop/conf/ranger-security.xml $CONF_DIR/ scp /etc/hadoop/conf/ranger-yarn-audit.xml $CONF_DIR/ scp /etc/hadoop/conf/ranger-yarn-security.xml $CONF_DIR/ ;} } sh scpCycle.sh /data/opt_cloudera/cm-agent/service/common/cloudera-config.sh sh cycle.sh \"chown cloudera-scm:cloudera-scm /data/opt_cloudera/cm-agent/service/common/cloudera-config.sh ;chmod 755 /data/opt_cloudera/cm-agent/service/common/cloudera-config.sh\" sh scpCycle.sh /opt/cloudera/parcels/CDH/lib/ranger-hive-plugin 之后还是报了另一个错： Error: Could not open client transport with JDBC Uri: jdbc:hive2://10.10.6.105:10000: Failed to open new session: java.lang.IllegalArgumentException: Cannot modify hive.query.redaction.rules at runtime. It is not in list of params that are allowed to be modified at runtime (state=08S01,code=0) 1 报错提及can not modify hive.query.redaction.rules 去查看这个在哪儿： cd /etc/hive grep -rni “hive.query.redaction.rules” 去掉配置文件中 hive-env.sh 中第7行加载了这个配置项hive.query.redaction.rules校验参数。 ### 以下为全部配完后 我找到的结果 [root@hshv01 ~]# find / -name \"ranger-hive-audit.xml\" /run/cloudera-scm-agent/process/1898-host-inspector/ranger-hive-audit.xml /run/cloudera-scm-agent/process/1863-impala-IMPALAD/ranger-hive-audit.xml /run/cloudera-scm-agent/process/1843-hive-HIVEMETASTORE/ranger-hive-audit.xml /run/cloudera-scm-agent/process/1842-hive-HIVESERVER2/ranger-hive-audit.xml /run/cloudera-scm-agent/process/1837-impala-IMPALAD/ranger-hive-audit.xml /run/cloudera-scm-agent/process/1816-hive-HIVESERVER2/ranger-hive-audit.xml /run/cloudera-scm-agent/process/1817-hive-HIVEMETASTORE/ranger-hive-audit.xml /etc/hive/conf.cloudera.hive/ranger-hive-audit.xml /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/ranger-hive-plugin/install/conf.templates/enable/ranger-hive-audit.xml CDH配置配置权限启用 继续参考上面 配0.5 ranger 的大佬，这里开始搞 hive 的权限开启 hive.security.authorization.enabled true hive.security.authorization.manager org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizerFactory hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator hive.conf.restricted.list hive.security.authorization.enabled,hive.security.authorization.manager,hive.security.authenticator.manager hive.security.authorization.sqlstd.confwhitelist.append mapred.*|hive.*|mapreduce.*|spark.*|var.*|flink.* hive.security.authorization.sqlstd.confwhitelist mapred.*|hive.*|mapreduce.*|spark.*|var.*|flink.* ———————————————— 版权声明：本文为CSDN博主「huangyq5240」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/huangyq5240/article/details/102721679 实际在 cdh 里 在 hive 高级里添加权限管理 在hive site 里有很多，对应 配置感觉是 用于 hive client 的权限封锁 ranger-admin 添加依赖包 配置ranger-admin 里添加 hive-dev test connection 失败 https://m.cppentry.com/bencandy.php?fid=117&id=186236 需要在 admin 的 下新增对应的 jdbc 的 lib包 （好像是 确实没有对应的包，感觉最好做软连接，回头有空再改） from_dir=/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars ## dest_dir=/data/ranger/ranger-2.0.0-admin/ews/webapp/WEB-INF/classes/ranger-plugins/ dest_dir=/app/ranger_admin/ews/webapp/WEB-INF/classes/ranger-plugins/ cp -r ${from_dir}/hive-common*.jar ${dest_dir}/hive cp -r ${from_dir}/hive-jdbc*.jar ${dest_dir}//hive cp -r ${from_dir}/hive-exec*.jar ${dest_dir}/hive cp -r ${from_dir}/hive-metastore*.jar ${dest_dir}/hive cp -r ${from_dir}/hive-service*.jar ${dest_dir}/hive scp -r ${from_dir}/hadoop-yarn-api-3.0.0-cdh6.3.2.jar ${dest_dir}/yarn/hadoop-yarn-api-3.0.0.jar scp -r ${from_dir}/hadoop-common-3.0.0-cdh6.3.2.jar ${dest_dir}/yarn/hadoop-common-3.0.0.jar scp -r ${from_dir}/hadoop-yarn-server-resourcemanager*.jar ${dest_dir}/yarn/hadoop-common-3.0.0.jar test 成功，save 时会转圈。时间很长（我是等了10分钟以上），等。 重启hiveserver2 cdh 上操作重启 hiveserver2 配置对应配置hive策略 yarn插件 把包放到cdh 这个目录下 直接粘的大佬的，目录 6.3.2 对应要换，主要就是 把 ranger-hive-plugin的包放到 cdh 里去 ## 解开包 改名字为 ranger-yarn-plugin tar -zxvf *yarn*.gz mv *yarn*-plugin ranger-yarn-plugin ## 放到对应的cdh目录下去 mv ranger-yarn-plugin /opt/cloudera/parcels/CDH/lib/ enable-agent.sh配置 问题描述 hdfs和yarn插件安装部署后，插件jar包会部署到组件安装目录的share/hadoop/hdfs/lib子目录下，启动hdfs或yarn运行时加载不到这些jar包，会报ClassNotFoundException: Class org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer not found 解决办法 修改agents-common模块enable-agent.sh脚本文件： 将 208 HCOMPONENT_LIB_DIR=${HCOMPONENT_INSTALL_DIR}/share/hadoop/hdfs/lib 234 HCOMPONENT_CONF_DIR=${HCOMPONENT_INSTALL_DIR}/config 修改为： 208 HCOMPONENT_LIB_DIR=${HCOMPONENT_INSTALL_DIR} 234 HCOMPONENT_CONF_DIR=${PROJ_LIB_DIR}/ranger-kafka-plugin-impl 改install.properties POLICY_MGR_URL=http://192.168.4.50:6080 REPOSITORY_NAME=yarndev ## 下面这个默认是hadoop ，我放到了cdh 的lib目录下，对于目录就是子目录 hadoop ，不用改 ## COMPONENT_INSTALL_DIR_NAME=/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop 我只改了这俩，没开solr 审计，没改用户，对应的 用户为默认 yarn ，组hadoop 启动 ./enable-yarn-plugin.sh 修改yarn-site.xml配置文件 YARN插件安装在所有ResourceManager节点服务器上 在CDH管理界面配置YARN参数，配置“yarn-site.xml 的 ResourceManager 高级配置代码段”，新增参数配置： yarn.authorization-provider=org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer cdh yarn 高级里添加 添加如下属性： yarn.acl.enable true yarn.authorization-provider org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer ranger搞api 包一层皮 Apache Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform. Apache Ranger currently provides a centralized security adminstration, fine grain access control and detailed auditing for user access within Apache Hadoop, Apache Hive, Apache HBase and other Apache components api目前只有0.6版本 https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.6+-+REST+APIs+for+Service+Definition,+Service+and+Policy+Management 2.1版本api http://ranger.apache.org/apidocs/ui/index.html USR_CONFIG=$1 RANGER_URL=22.188.12.73:6080 ADMIN_PASSWD=Bdp@boc01! ## 发送创建用户curl -u admin:${ADMIN_PASSWD} -v -i -s -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://${RANGER_URL}/service/xusers/secure/users -d@${USR_CONFIG} ## 创建组 curl -u admin:Bdp@boc01! -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X POST 'http://22.188.12.73:6080/service/xusers/secure/groups' -d '{\"name\":\"iisp22\",\"description\":\"iisp - add from bft_manager\"}' curl -u admin:Bdp@boc01! --request POST 'http://22.188.12.73:6080/service/xusers/secure/groups' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"name\":\"iisp\",\"description\":\"iisp - add from bft_manager\"}' groupSource:0表示Internal，1表示External 创建用户 curl -u admin:Bdp@boc01! -v -i -s -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://22.188.12.73:6080/service/xusers/secure/users -d@userfile.json curl -u admin:Bdp@boc01! -v -i -s -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://22.188.12.73:6080/service/xusers/secure/users -d '{ \"name\":\"mqddd\", \"firstName\":\"mqeee\", \"lastName\": \"mqccc\", \"loginId\": \"mqccc\", \"emailAddress\" : null, \"description\" : \"mqccc - add fromambari\", \"password\" : \"Bdp@boc01!\", \"groupIdList\":[ \"hadoop\" ], \"status\":1, \"isVisible\":1, \"userRoleList\": [ \"ROLE_USER\" ], \"userSource\":1 }' 注明：@userfile.json文件也可以用json格式字符串替换 -d ‘{... ...}’ 注明：某些接口可能需要移除/secure 注明2：userfile.json内容举例如下 { \"name\":\"mqddd\", \"firstName\":\"mqccc\", \"lastName\": \"mqccc\", \"loginId\": \"mqccc\", \"emailAddress\" : null, \"description\" : \"mqccc - add fromambari\", \"password\" : \"Bdp@boc01!\", \"groupIdList\":[\"hadoop\"], \"status\":1, \"isVisible\":1, \"userRoleList\": [ \"ROLE_USER\" ], \"userSource\":1 } userRoleList取值：\"ROLE_SYS_ADMIN\" 或 \"ROLE_USER\" 移除密码字段，用户将变成External用户 \"userSource\":0表示Internal，1表示External 创建policy ## 登陆模式 curl --location --request POST 'http://22.188.12.73:6080/service/plugins/policies' \\ --header 'Content-Type: application/json' \\ --header 'Cookie: RANGERADMINSESSIONID=1EAD5E79135EE8524DFE1B7AC93D62DC' \\ --data-raw '{\"policyType\":\"0\",\"name\":\"ssss\",\"isEnabled\":true,\"policyPriority\":0,\"policyLabels\":[],\"description\":\"\",\"isAuditEnabled\":true,\"resources\":{\"database\":{\"values\":[\"bdp\"],\"isRecursive\":false,\"isExcludes\":false},\"table\":{\"values\":[\"mqtest\"],\"isRecursive\":false,\"isExcludes\":false},\"column\":{\"values\":[\"col2\"],\"isRecursive\":false,\"isExcludes\":false}},\"isDenyAllElse\":false,\"policyItems\":[{\"users\":[\"bdp\"],\"accesses\":[{\"type\":\"select\",\"isAllowed\":true},{\"type\":\"update\",\"isAllowed\":true},{\"type\":\"create\",\"isAllowed\":true},{\"type\":\"drop\",\"isAllowed\":true},{\"type\":\"alter\",\"isAllowed\":true},{\"type\":\"index\",\"isAllowed\":true},{\"type\":\"lock\",\"isAllowed\":true},{\"type\":\"all\",\"isAllowed\":true},{\"type\":\"read\",\"isAllowed\":true},{\"type\":\"write\",\"isAllowed\":true},{\"type\":\"repladmin\",\"isAllowed\":true},{\"type\":\"serviceadmin\",\"isAllowed\":true},{\"type\":\"tempudfadmin\",\"isAllowed\":true},{\"type\":\"refresh\",\"isAllowed\":true}]}],\"allowExceptions\":[],\"denyPolicyItems\":[],\"denyExceptions\":[],\"service\":\"hivedev\"} ' ## 密钥模式 curl -u admin:Bdp@boc01! --request POST 'http://22.188.12.73:6080/service/plugins/policies' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"policyType\":\"0\",\"name\":\"ssss2\",\"isEnabled\":true,\"policyPriority\":0,\"policyLabels\":[],\"description\":\"\",\"isAuditEnabled\":true,\"resources\":{\"database\":{\"values\":[\"bdp222\"],\"isRecursive\":false,\"isExcludes\":false},\"table\":{\"values\":[\"mqtest\"],\"isRecursive\":false,\"isExcludes\":false},\"column\":{\"values\":[\"col2\"],\"isRecursive\":false,\"isExcludes\":false}},\"isDenyAllElse\":false,\"policyItems\":[{\"users\":[\"bdp\"],\"accesses\":[{\"type\":\"select\",\"isAllowed\":true},{\"type\":\"update\",\"isAllowed\":true},{\"type\":\"create\",\"isAllowed\":true},{\"type\":\"drop\",\"isAllowed\":true},{\"type\":\"alter\",\"isAllowed\":true},{\"type\":\"index\",\"isAllowed\":true},{\"type\":\"lock\",\"isAllowed\":true},{\"type\":\"all\",\"isAllowed\":true},{\"type\":\"read\",\"isAllowed\":true},{\"type\":\"write\",\"isAllowed\":true},{\"type\":\"repladmin\",\"isAllowed\":true},{\"type\":\"serviceadmin\",\"isAllowed\":true},{\"type\":\"tempudfadmin\",\"isAllowed\":true},{\"type\":\"refresh\",\"isAllowed\":true}]}],\"allowExceptions\":[],\"denyPolicyItems\":[],\"denyExceptions\":[],\"service\":\"hivedev\"} ' 获取用户信息 根据用户名查询，不包含与group关系 curl -u admin:admin -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://:6080/service/xusers/users/userName/zhouyuan 根据用户ID查询，包含与group关系（可以根据用户名查询获取用户ID） curl -u admin:admin -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://:6080/service/xusers/secure/users/{id} 删除用户 软删除(即将用户状态设为Hidden) curl -u admin:admin -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X DELETEhttp://:6080/service/xusers/users/userName/zhouyuan 硬删除 URL末尾添加参数：forceDelete=true，即 curl -u admin:admin -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X DELETE http://:6080/service/xusers/users/userName/zhouyuan?forceDelete=true 查询组 模糊搜索(可能返回列表)： curl -u admin:admin -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://:6080/service/xusers/groups?name=group1 精确获取此群信息： curl -u admin:admin -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GEThttp://:6080/service/xusers/groups/groupName/group1 删除组 curl -u admin:admin -v -i -s -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X DELETE http://:6080/service/xusers/groups/groupName/{groupName} 同删除用户一样，分软删除和硬删除 组更新用户成员 1、添加方式：Ambari中组添加用户，Ranger用户添加组 2、Ambari已采用python脚本实现了与LDAP用户同步 3、Ambari用户变更前后，识别哪些用户新增到此组，哪些用户不变（不管），哪些用户解绑此组。（需要查询一次进行修改前后用户对比） 4、每个新增/解绑用户调用ranger接口，获取用户信息从而知道用户{id}，获取群组{id}, put消息修改用户groupIdList字段（其他字段不变） curl -u admin:admin -v -i -s -X PUT -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://:6080/service/xusers/secure/users/{id} -d@userfile.json "},"知识笔记/大数据组件/数据安全与权限管理/数据探查.html":{"url":"知识笔记/大数据组件/数据安全与权限管理/数据探查.html","title":"数据探查","keywords":"","body":"数据质量探查 https://blog.csdn.net/weixin_42893650/article/details/90640708 "},"知识笔记/大数据组件/数据安全与权限管理/数据质量管理-griffin.html":{"url":"知识笔记/大数据组件/数据安全与权限管理/数据质量管理-griffin.html","title":"数据质量管理-griffin","keywords":"","body":""},"知识笔记/大数据组件/数据采集与传输/sqoop/sqoop.html":{"url":"知识笔记/大数据组件/数据采集与传输/sqoop/sqoop.html","title":"Sqoop","keywords":"","body":"SQOOP sqoop部署 sqoop 部署依赖对应的jdbc连接jar包 连接 oracle 需要 oracle 的jar包 ojdbc6.jar cp ojdbc6.jar /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/sqoop/lib/ sqoop从oracle抽到hive_orc hive 默认是指的文本这个格式，其他的格式都要用到 hcatalog https://www.cnblogs.com/EnzoDin/p/10653350.html 下面的是into 这种 updated=\"col1 > 3 \" sqoop import -D mapred.job.queue.name=root.default \\ -D mapred.job.name=SQOOP \\ --connect jdbc:oracle:thin:@//22.188.12.78:1521/orazyxf \\ --username blck \\ --password blck01! \\ --query \"SELECT * FROM BLCK.MQTEST WHERE ${updated} AND \\$CONDITIONS\" \\ --hcatalog-database bdp \\ --hcatalog-table mqtest_orc \\ --hcatalog-partition-keys day \\ --hcatalog-partition-values 20210201 \\ -m 1 这个 $CONDITIONS 是 1=0 一定要填。 如果是oracle rac 对应的链接串需要修改，目前这个是 service 形式的串，如果是其他TNS 的串的化也要对应修改 官方文档说 hcatalog这个模式不能做overwrite Unsupported Sqoop Hive Import Options The following Sqoop Hive import options are not supported with HCatalog jobs. --hive-import --hive-overwrite 如果是要做 overwrite hdfs dfs -rm /user/hive/warehouse/bdp.db/mqtest_orc/day=20210201/ 可选参数 --table BLCK.MQTEST 官方文档 http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_new_command_line_options 已知约束 hive分区字段问题 sqoop通过上述命令导数到hive分区表时，分区字段为 varchar时会出现异常。 建议分区字段使用 String "},"知识笔记/大数据组件/架构设计/oppo实时大数据平台衍进.html":{"url":"知识笔记/大数据组件/架构设计/oppo实时大数据平台衍进.html","title":"oppo实时大数据平台衍进","keywords":"","body":"很有意义的架构设计思路 基于 flink https://blog.csdn.net/u013411339/article/details/99912664 "},"知识笔记/大数据组件/计算引擎/mr/mapreduce.html":{"url":"知识笔记/大数据组件/计算引擎/mr/mapreduce.html","title":"Mapreduce","keywords":"","body":"MR 调用 mr example example 里除了wordcount 还有很多可以用的代码。 参考 https://www.cnblogs.com/sundy818/p/10082504.html "},"知识笔记/大数据组件/计算引擎/spark/spark.html":{"url":"知识笔记/大数据组件/计算引擎/spark/spark.html","title":"Spark","keywords":"","body":"spark spark pi example spark-submit --master yarn --class org.apache.spark.examples.SparkPi /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/examples/jars/spark-examples_2.11-2.4.0-cdh6.3.2.jar 100 "},"知识笔记/大数据组件/资源调度/yarn/cdh yarn 配置.html":{"url":"知识笔记/大数据组件/资源调度/yarn/cdh yarn 配置.html","title":"cdh yarn 配置","keywords":"","body":""},"知识笔记/大数据组件/资源调度/yarn/yarn-cdh队列隔离.html":{"url":"知识笔记/大数据组件/资源调度/yarn/yarn-cdh队列隔离.html","title":"yarn-cdh队列隔离","keywords":"","body":"CDH-队列隔离 https://blog.csdn.net/zhangshenghang/article/details/94554469 参考大佬的放置规则 https://cloud.tencent.com/developer/article/1361507 cm动态资源池放置规则 仅当池 已在运行时指定 存在时使用该池。 标志如果用户指定了对应的资源池时，使用用户指定的资源池。 如果指定的资源池不存在，则报错 Use the pool root.users.[username] only if the pool and root.users both exists. 没有指定资源池时， mqtest 用户提交的任务，默认使用类似 root.users.mqtest 对应的资源池需要存在 关于放置规则类型的解释说明： root.[pool name]:该规则始终满足，在其它规则不匹配的情况下使用，因此该规则默认要放置在所有匹配规则之后。 root.[pool name].[username]：该放置规则会判断资源池中是否存在相应的pool name，存在则在该资源池下创建与用户名相同的资源池（勾选池不存在时创建池的情况下）。 root.[primary group]：该放规则使用与该用户主要组匹配的资源池。Linux中用户默认的主要组与用户名一致，匹配时会通过用户的主要组与资源池名称比对。 root.[primary group].[username]：该放置规则会优先使用用户的主要组匹配的资源池，然后使用与该用户名匹配的子池，如果勾选池不存在时创建池则会在该池下创建一个与用户名一致的子池。 root.[secondarygroup]：该放置规则用于匹配用户的次要组，使用与次要组之一匹配的资源池。 root.[secondarygroup].[username]：该放置规则首先匹配用户的次要组，然后使用与该用户名匹配的资源池。 root.[username]:该放置规则用于匹配与用户名一致的资源池。（不推荐使用） 已在运行时指定：该放置规则主要使用在运行时指定的资源池。 放置规则的判断方式，根据放置规则的顺序1、2、3…进行判断，判断到满足条件的放置规则后，后续的规则不再进行匹配。 "},"知识笔记/版本管理/git/git.html":{"url":"知识笔记/版本管理/git/git.html","title":"Git","keywords":"","body":"git [toc] 特点概述 分散型版本管理 markdown 标准化编程 与 社会化编程 git mermaid graph TD subgraph remote github G1(git/mq5445/etljc/master) G2(git/mq5445/etljc/develop) G3(git/mq5445/etljc/feature/xx) GR(git/mq5445/etljc/release/1.0.0) GH(git/mq5445/etljc/hotfix) end subgraph local git %% L1(etljc/master) L2(etljc/develop) L3(etljc/feature/xx) end subgraph upsteam github R1(git/BOC/etljc/master) end subgraph test env T1(env compile and test) end G1--branch-->G2 %% G1--clone-->L1 %% L1--push-->G1 %% L1--checkout-->L2 G2--clone/pull-->L2 L2--gitflow_feature_start-->L3 L3--push-->G3 G1--pull_reqest-->R1 R1--fork-->G1 G3--pull_reqest-->G2 G2--gitflow_release-->GR GR-->T1 T1--bug_fix-->GR GR--finish_and_tag_master-->G1 G1--gitflow_hotfix_start-->GH-->T1 T1--bug_fix-->GH GH--finish_and_tag_master-->G1 git 中文异常 https://blog.csdn.net/jfsufeng/article/details/79219673 git config --global core.quotepath false 中文手册 https://www.php.cn/manual/view/35088.html 本地英文手册 file:///D:/software/Git/mingw64/share/doc/git-doc/user-manual.html github图片不显示 199.232.68.133 raw.githubusercontent.com 清理历史 https://docs.github.com/en/github/authenticating-to-github/removing-sensitive-data-from-a-repository 配置多 ssh 密钥 参考文档 https://blog.csdn.net/qq_30227429/article/details/80229167 https://blog.csdn.net/weixin_33749242/article/details/89023331 生成一个新的密钥 ssh-keygen -t rsa -f ~/.ssh/id_rsa_gitlab1 -C \"mq5445@mail.notes.bank-of-china.com\" 在 .ssh 文件夹下新建 config 文件并编辑，另不同 Host 实际映射到同一 HostName，但密钥文件不同。Host 前缀可自定义，例子中ieit 妈的，被这个gitbash 坑了。 网上所有写 ~/.ssh/config 估计都不是windows ，至少不是 win gitbash下的吧。 可别信别人都有，我没有，那我就 vi 建一个吧 搞了两个小时。一直显示连不上，各种调。 看了下 gitbash 里的ssh 配置到底在哪。 $ ssh -vT git@github.com OpenSSH_8.0p1, OpenSSL 1.1.1c 28 May 2019 debug1: Reading configuration data /etc/ssh/ssh_config debug1: Connecting to github.com [52.74.223.119] port 22. 在 /etc/ssh 看看正经的目录有啥 $ ll /etc/ssh/ total 572 -rw-r--r-- 1 miaoq 197609 577388 6月 8 2019 moduli -rw-r--r-- 1 miaoq 197609 1556 6月 8 2019 ssh_config -rw-r--r-- 1 miaoq 197609 3122 6月 8 2019 sshd_config ssh_config 是个客户端配置 sshd_config 是个服务器配置 vi ~/.ssh/config # github user Host git@github.com Hostname https://github.com/ User bulbcat IdentityFile ~/.ssh/id_rsa #gitlab user Host gitlab Hostname http://22.11.38.153 User mq5445 IdentityFile ~/.ssh/id_rsa_gitlab1 "},"知识笔记/版本管理/git/gitbash中文异常.html":{"url":"知识笔记/版本管理/git/gitbash中文异常.html","title":"gitbash中文异常","keywords":"","body":"gitbash 中文异常 "},"知识笔记/版本管理/行内cc/出版本.html":{"url":"知识笔记/版本管理/行内cc/出版本.html","title":"出版本","keywords":"","body":"中行出版本 cq 配置基线 只涉及到文档 "},"知识笔记/甘特图/甘特图.html":{"url":"知识笔记/甘特图/甘特图.html","title":"甘特图","keywords":"","body":"甘特图 https://baijiahao.baidu.com/s?id=1644151983194720829&wfr=spider&for=pc "},"知识笔记/知识管理/gitbook/gitbook.html":{"url":"知识笔记/知识管理/gitbook/gitbook.html","title":"Gitbook","keywords":"","body":"gitbook GitBook 是一个 Node.js 环境下，用于构建电子书的工具。 感觉和gitpage 联合使用会很好用。 参考 https://www.jianshu.com/p/3d03ab330df5 gitbook 要装 node js gitbook 和 git 没啥关系， gitbook是个管理markdown文件并可以构建发布的工具 对应成品 如果上传github 的话，要对应选中 ignore 参考文档 https://www.jianshu.com/p/421cc442f06c 里面的 gitboot -v 这句似乎有问题 gitboot --verison gitbook-plugin 安装插件参考 https://www.cnblogs.com/mingyue5826/p/10307051.html https://segmentfault.com/a/1190000019806829 装东西 ## 中文搜索 npm install gitbook-plugin-search-pro ## mermaid npm install gitbook-plugin-mermaid-gb3 ## 折叠目录 npm install gitbook-plugin-chapter-fold npm install gitbook-plugin-expandable-chapters npm install gitbook-plugin-expandable-chapters-small ## 回到顶部 npm install gitbook-plugin-back-to-top-button ## 可调节侧边栏 npm install gitbook-plugin-splitter 添加到配置里 vim book.json { \"plugins\": [\"splitter\",\"-lunr\",\"mermaid-gb3\",\"chapter-fold\",\"expandable-chapters\",\"expandable-chapters-small\",\"-search\", \"search-pro\",\"back-to-top-button\"], \"pluginsConfig\": { \"disqus\": { \"shortName\": \"druler\" } } } gitbook-plugin-summury 用以自动生成 summary 的工具 http://self-publishing.ebookchain.org/3-%E5%A6%82%E4%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B9%B3%E5%8F%B0%EF%BC%9F/2-Summary%E7%9A%84%E4%BD%BF%E7%94%A8.html gitbook本地启动 gitbook build gitbook serve "},"知识笔记/知识管理/mermaid/甘特图_Gantt.html":{"url":"知识笔记/知识管理/mermaid/甘特图_Gantt.html","title":"甘特图_Gantt","keywords":"","body":"甘特图 "},"知识笔记/知识管理/typora/ppt_demo.html":{"url":"知识笔记/知识管理/typora/ppt_demo.html","title":"Ppt Demo","keywords":"","body":" # Markdown # 制作幻灯片 ## by 契卡 "},"知识笔记/知识管理/typora/typora配置.html":{"url":"知识笔记/知识管理/typora/typora配置.html","title":"typora配置","keywords":"","body":"Typora 配置 [toc] 改了css 样式 搞了下 一级标题 不加序号 目录的字体颜色 保持同步 C:\\Users\\miaoq\\AppData\\Roaming\\Typora\\themes 加了preference,显示代码行数 md画图 st=>start: Start st2=>start: start2 in1=>inputoutput: input1 in2=>inputoutput: input2 op=>operation: Your Opera op2=>operation: xxxx cond=>condition: Yes or No? e=>end st->in1->op->cond st2->in2->op cond(yes)->e cond(no)->op graph LR; master-->portal 　　client---core; 　　client---common; 　　core---common; 　　common---portal; 　　common---Biz; 　　Biz---ConfigService; 　　Biz---AdminService; 试着画etljc graph TD dcds文本--ftp-->ftp subgraph FOP ftp--识别文件规则 异常-->lost ftp--识别文件规则 正常-->recdir--解压-->ugdir--本地文件备份-->bakdir end subgraph etljc-master subgraph cycle1-文本守护 ugdir--datax上传 记录日志进mysql-->sys_file_stat sys_file_cfg-.输入.->cycle1工作守护 sys_file_stat-.输入.->cycle1工作守护--满足条件 更新-->sys_file_stat end subgraph cycle2-表守护 cycle1工作守护--满足条件 插入-->sys_table_stat sys_table_stat-.输入.->cycle2工作守护--条件满足判断 是-->分发调度--资源满足判断-->判断处理模式 cycle2工作守护--满足条件 更新-->sys_table_stat sys_router-.输入.->分发调度 sys_table_cfg-.输入.->判断处理模式 判断处理模式-->时点表时段表 判断处理模式-->时点增量表 end end subgraph hdfs ugdir--datax-->RAW MID ODS end subgraph SCH subgraph 时点表时段表处理流程 %% RAW-.输入.->trim 时点表时段表--按资源饱和度分发-->trim--更新-->sys_table_stat trim--写入-->ODS end subgraph 时点增量表处理流程 %% RAW-.输入.->trim* 时点增量表--按资源饱和度分发-->trim*--更新-->sys_table_stat trim*--写入-->MID trim*--master节点满足条件判定-->merge--写入-->ODS end end 图片设置相对路径 相对路径后，再分享模式会容易很多。直接扔对应的文件夹就可以用了。 git 上也可以直接引用。 目前设置在了 ./.assets 用于方便编辑的 比如 excel 放进了 ./.excel 目录。方便对数据的整理。 利用 . 使目录变为隐藏目录，typora文件树中不会显示，更方便的阅读条目。 如下图 导出pdf 对应图片质量会下降 pdf 实测 大小很小，对应图片质量需要搭开确认可用情况 "},"知识笔记/网络协议/postman/postman.html":{"url":"知识笔记/网络协议/postman/postman.html","title":"Postman","keywords":"","body":"postman 登陆： http://22.188.12.73:6080/login?username=admin&password=Bdp@boc01! 下午04:44:50: Content-Type application/x-www-form-urlencoded; charset=UTF-8 下午04:45:48: 添加： 下午04:45:55: http://22.188.12.73:6080/service/plugins/policies 下午04:46:11: Content-Type application/json 下午04:46:59: {\"policyType\":\"0\",\"name\":\"ssss\",\"isEnabled\":true,\"policyPriority\":0,\"policyLabels\":[],\"description\":\"\",\"isAuditEnabled\":true,\"resources\":{\"database\":{\"values\":[\"bdp\"],\"isRecursive\":false,\"isExcludes\":false},\"table\":{\"values\":[\"mqtest\"],\"isRecursive\":false,\"isExcludes\":false},\"column\":{\"values\":[\"col2\"],\"isRecursive\":false,\"isExcludes\":false}},\"isDenyAllElse\":false,\"policyItems\":[{\"users\":[\"bdp\"],\"accesses\":[{\"type\":\"select\",\"isAllowed\":true},{\"type\":\"update\",\"isAllowed\":true},{\"type\":\"create\",\"isAllowed\":true},{\"type\":\"drop\",\"isAllowed\":true},{\"type\":\"alter\",\"isAllowed\":true},{\"type\":\"index\",\"isAllowed\":true},{\"type\":\"lock\",\"isAllowed\":true},{\"type\":\"all\",\"isAllowed\":true},{\"type\":\"read\",\"isAllowed\":true},{\"type\":\"write\",\"isAllowed\":true},{\"type\":\"repladmin\",\"isAllowed\":true},{\"type\":\"serviceadmin\",\"isAllowed\":true},{\"type\":\"tempudfadmin\",\"isAllowed\":true},{\"type\":\"refresh\",\"isAllowed\":true}]}],\"allowExceptions\":[],\"denyPolicyItems\":[],\"denyExceptions\":[],\"service\":\"hivedev\"} "},"知识笔记/网络协议/rest/rest.html":{"url":"知识笔记/网络协议/rest/rest.html","title":"Rest","keywords":"","body":"Rest https://zhuanlan.zhihu.com/p/52925075 面向资源的，一个资源被一个url指代 "},"知识笔记/学习路线图.html":{"url":"知识笔记/学习路线图.html","title":"学习路线图","keywords":"","body":"学习路线图 [toc] java java SE jvm 框架 linux 常用工具 版本管理 git git-flow 依赖管理 maven 大数据组件 存储 hdfs namenode 内存占用原理 kudu 资源调度 yarn mesos 计算引擎 spark mr tez flink 分布式协调服务 zookeeper euraka zuul 消息服务 kafka 结构化数据仓库 hive 数据采集与传输 flume logstack datax sqoop OLAP-MPP presto impala OLAP-其他 clickhouse apache-druid kylin nosql hbase mongodb redis 图数据库 neo4j 数据治理 ranger(权限管理) atlas(血缘管理) griffin(质量管理) 搜索 solr es 任务调度 oozie airflow azkaban kettle etljc等各种其他自研工具 感兴趣的项目 streamlit streamlit 基于 JavaScript 实现的轻量级 Web 电子表格库。它功能齐全，包含表格的基本操作和函数等，还有详细的中文文档，在线尝试 https://github.com/MarkerHub/vueblog "}}